<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Ch19: Approximate Inference | Notes</title>
  <meta name="author" content="Yutaro Yamada">
  
  <meta name="description" content="Notes for myself: Ch19 Approximate Inference http://www.deeplearningbook.org/contents/inference.html
Introduction
Inference = compute $P(h|v)$.

There">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Ch19: Approximate Inference"/>
  <meta property="og:site_name" content="Notes"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/blog/favicon.png" rel="icon">
  <link rel="alternate" href="/blog/atom.xml" title="Notes" type="application/atom+xml">
  <link rel="stylesheet" href="/blog/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/blog/">Notes</a></h1>
  <h2><a href="/blog/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/blog/">Home</a></li>
    
      <li><a href="/blog/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-06T09:17:33.000Z"><a href="/blog/2016/06/06/Ch19-Approximate-Inference/">2016-06-06</a></time>
      
      
  
    <h1 class="title">Ch19: Approximate Inference</h1>
  

    </header>
    <div class="entry">
      
        <p>Notes for myself: <strong>Ch19</strong> <strong>Approximate Inference</strong> <a href="http://www.deeplearningbook.org/contents/inference.html" target="_blank" rel="external">http://www.deeplearningbook.org/contents/inference.html</a></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul>
<li><p>Inference = compute $P(h|v)$.</p>
</li>
<li><p>There are many situations where you want to calculate the posterior distribution $P(h|v)$ (i.e. sparse coding, ). Here, h is a set of hidden variables and v is a set of observed variables. In general, when the model is complicated, it is intractable to compute the corresponding $P(h|v)$. This chapter introduces many examples of the inference problem, which approximates $P(h|v)$.   </p>
</li>
</ul>
<h3 id="19-1-Inference-as-Optimization"><a href="#19-1-Inference-as-Optimization" class="headerlink" title="19.1 Inference as Optimization"></a>19.1 Inference as Optimization</h3><ul>
<li><p>The core idea of approximate inference.</p>
</li>
<li><p>Given a probabilistic model with latent variables $h$ and observed variables $v$, we want to know how good it is. One measure of goodness is $\log p(v; \theta)$, often called model evidence or marginalized likelihood.</p>
</li>
<li><p>When integrating out $h$ is difficult, we instead seek for an alternative. Is there a way to lower bound $\log p(v, \theta)$?</p>
</li>
<li><p>Consider the following:<br>$$<br>L(v, \theta, q) = \log p(v; \theta) - KL(q(h|v), p(h|v; \theta))<br>$$</p>
</li>
<li><p>Since KL divergence is always non-negative, we can think of L(v, \theta, q) as a lower bound approximation of $\log p(v, \theta)$.  </p>
</li>
<li><p>By modifying the above equation, we have</p>
<span>$$L(v,\theta,q) = \log p(v;\theta) - E_{h \sim q}\log \frac{q(h|v)}{p(h|v;\theta)} \\
= \log p(v; \theta) - E_{h \sim q}\log\frac{q(h|v)}{\frac{p(v,h; \theta)}{p(v; \theta)}} \\
= \log p(v; \theta) - E_{h\sim q}[ \log q(h|v) - \log \frac{p(v,h; \theta)}{p(v; \theta)}] \\
= - E_{h\sim q}[ \log q(h|v) - \log p(v,h; \theta)] \\
= - E_{h\sim q}[ \log q(h|v)] + E_{h \sim q}[\log p(v,h; \theta)] \\
= E_{h \sim q}[\log p(v,h; \theta)] + H(q)$$</span><!-- Has MathJax -->
<p>where <span>$H(q) = - E_{h\sim q}[ \log q(h|v)]$</span><!-- Has MathJax -->.</p>
</li>
<li><p>For an appropriate choice of $q$, $L$ is tractable.</p>
</li>
<li><p>The following sections will show how to derive different forms of approximate inference by using approximate optimization to find $q$.</p>
</li>
</ul>
<h3 id="19-2-Expectation-Minimization"><a href="#19-2-Expectation-Minimization" class="headerlink" title="19.2 Expectation Minimization"></a>19.2 Expectation Minimization</h3><p>(I thought it’s easier to view this section through an example so I’ll add some Mixture-of-Gaussian taste in my notes)</p>
<ul>
<li>The goal is to maximize the lower bound $L(v,\theta, q)$. Note that we have two distinct terms when we expand $L$ in the above section: the left term is parameterized by $\theta$ and the right term is parameterized by $q$.</li>
</ul>
<h3 id="19-3-MAP-Inference-and-Sparse-Coding"><a href="#19-3-MAP-Inference-and-Sparse-Coding" class="headerlink" title="19.3 MAP Inference and Sparse Coding"></a>19.3 MAP Inference and Sparse Coding</h3><p>(will skip this for the time being)</p>
<h3 id="19-4-Variational-Inference-and-learning"><a href="#19-4-Variational-Inference-and-learning" class="headerlink" title="19.4 Variational Inference and learning"></a>19.4 Variational Inference and learning</h3><ul>
<li>The core idea: maximize $L$ over a restricted family of distributions $q$.</li>
</ul>
<h3 id="19-5-Learned-Approximate-Inference"><a href="#19-5-Learned-Approximate-Inference" class="headerlink" title="19.5 Learned Approximate Inference"></a>19.5 Learned Approximate Inference</h3><ul>
<li>The core idea: we can view the iterative optimization of maximizing $L(v,q)$ w.r.t. $q$ as a function $f$ that maps an input $v$ to an approximate distribution $q* = argmax_q L(v,q)$. Once we view this way, we can learn a function $f(v; \theta)$ with neural network.   </li>
</ul>
<h2 id="Ch14-Autoencoders"><a href="#Ch14-Autoencoders" class="headerlink" title="Ch14 : Autoencoders"></a>Ch14 : Autoencoders</h2><h3 id="14-4-Stochastic-Encoders-and-Decoders"><a href="#14-4-Stochastic-Encoders-and-Decoders" class="headerlink" title="14.4 Stochastic Encoders and Decoders"></a>14.4 Stochastic Encoders and Decoders</h3><h2 id="Ch3-Probability-and-Information-Theory"><a href="#Ch3-Probability-and-Information-Theory" class="headerlink" title="Ch3 : Probability and Information Theory"></a>Ch3 : Probability and Information Theory</h2><p><a href="http://www.deeplearningbook.org/contents/prob.html" target="_blank" rel="external">http://www.deeplearningbook.org/contents/prob.html</a></p>
<h3 id="3-13-Information-Theory"><a href="#3-13-Information-Theory" class="headerlink" title="3.13 Information Theory"></a>3.13 Information Theory</h3><p><em>Shannon entropy</em> = $H(x)$ = $-E_{x \sim P}[\log P(x)]$ (also denoted $H(P)$)</p>
<p>In words, the Shannon entropy of a distribution $P$ is the expected amount of information in an event drawn from that distribution.</p>
<p><em>KL divergence</em> = $D<em>{KL}[P || Q]$ = $E</em>{x \sim P}[\log P(x) - \log Q(x)]$</p>
<p><em>Cross entropy</em> = $H(P,Q)$ = $H(P) + D<em>{KL}(P||Q)$ = $-E</em>{x \sim P}\log Q(x)$</p>
<p>Minimizing the cross-entropy w.r.t Q is equivalent to minimizing the KL divergence.</p>
<h2 id="Ch5-Machine-Learning-Basics"><a href="#Ch5-Machine-Learning-Basics" class="headerlink" title="Ch5 : Machine Learning Basics"></a>Ch5 : Machine Learning Basics</h2><h3 id="5-5-Maximum-Likelihood-estimation"><a href="#5-5-Maximum-Likelihood-estimation" class="headerlink" title="5.5 Maximum Likelihood estimation"></a>5.5 Maximum Likelihood estimation</h3><p>-</p>

      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/blog/tags/math/">math</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Kommentare</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="http://runopti.github.io/blog/2016/06/06/Ch19-Approximate-Inference/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Suche">
    <input type="hidden" name="q" value="site:runopti.github.io/blog">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/blog/tags/PRML/">PRML</a><small>1</small></li>
  
    <li><a href="/blog/tags/cv/">cv</a><small>1</small></li>
  
    <li><a href="/blog/tags/math/">math</a><small>5</small></li>
  
    <li><a href="/blog/tags/neuralnet/">neuralnet</a><small>5</small></li>
  
    <li><a href="/blog/tags/optimization/">optimization</a><small>3</small></li>
  
    <li><a href="/blog/tags/paper-memo/">paper-memo</a><small>2</small></li>
  
    <li><a href="/blog/tags/research/">research</a><small>0</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2017 Yutaro Yamada
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/blog/js/jquery.imagesloaded.min.js"></script>
<script src="/blog/js/gallery.js"></script>




<link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/blog/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]],"processEscapes":true}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
