<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Nonparametric Regression 01 | Notes</title>
  <meta name="author" content="Yutaro Yamada">
  
  <meta name="description" content="The goal of this post is to show an instance of derivation of risk bound for nonparmetric regression.   
ModelFor simplicity, we will focus on Gaussia">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Nonparametric Regression 01"/>
  <meta property="og:site_name" content="Notes"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/blog/favicon.png" rel="icon">
  <link rel="alternate" href="/blog/atom.xml" title="Notes" type="application/atom+xml">
  <link rel="stylesheet" href="/blog/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/blog/">Notes</a></h1>
  <h2><a href="/blog/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/blog/">Home</a></li>
    
      <li><a href="/blog/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-13T00:56:57.000Z"><a href="/blog/2017/03/12/nonparametric-estimation/">2017-03-12</a></time>
      
      
  
    <h1 class="title">Nonparametric Regression 01</h1>
  

    </header>
    <div class="entry">
      
        <p>The goal of this post is to show an instance of derivation of risk bound for nonparmetric regression.   </p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>For simplicity, we will focus on Gaussian sequence models. That is, our model of interest is the following type:</p>
<span>$$Y_i = \theta_i + \frac{1}{\sqrt{n}} \sigma Z_i, \text{ where } Z_i \sim^{iid} N(0,1)$$</span><!-- Has MathJax -->
<p>for $i = 1, …, n$, where we assume $\sigma=1$. In another word, we observe a noisy parameter $Y$, and we are interested in denoising $Y$ to get the true parameter $\theta$. </p>
<h2 id="Parameter-space"><a href="#Parameter-space" class="headerlink" title="Parameter space"></a>Parameter space</h2><p>Our parameter space is a type of Sobolev ellipsoid:<br><span>$$\Theta = \{ \theta : \sum_{i=1}^{\infty} i^{2 \alpha} \theta^2_i \le M \}$$</span><!-- Has MathJax --> </p>
<p>This way of choose our parameter space implicitly imposes smoothness assumption on function space we might be interested in. For example, if we view our estimation problem as a function estimation, the problem becomes to estimate a function $f(x)$ through a set of discrete sample pairs $(X_i, Y_i)$.<br>By Fourier basis expansion, </p>
<span>$$f(x) = \sum_{i=1}^{\infty} &lt;f, \varphi_i&gt; \varphi_i(x) \\
= \sum_{i=1}^{\infty} \theta_i \varphi_i(x)$$</span><!-- Has MathJax -->
<p>where <span>$\theta_i =  &lt;f, \varphi_i&gt;$</span><!-- Has MathJax --> and</p>
<span>$$\varphi_i(x) = 
\begin{cases} 
    cos(cix) \\
    sin(cix)
\end{cases}$$</span><!-- Has MathJax -->
<p>Imposing smoothness assumption on $f$ by bounding the integral of $f$’s second derivative is equivalent to the condition in our parameter space. That is, </p>
<span>$$f&apos;&apos;(x) = \sum_{i=1}^{\infty} \theta_i (\varphi_i(x))&apos;&apos; \\
= \sum_{i=1}^{\infty} \theta_i (ci)^2 \varphi_i(x)$$</span><!-- Has MathJax -->  
<p>So</p>
<span>$$\int (f&apos;&apos;(x))^2 dx \le M&apos; \iff \sum_{i=1}^{\infty} [\theta_i (ci)^2]^2 \le M&apos; \\
\sum_{i=1}^{\infty} c^4 i^4 \theta_i^2 \le M&apos;$$</span><!-- Has MathJax -->
<p>(In this derivation, we’ve assumed $\alpha=2$.)</p>
<p>Note that without any parameter space condition, the best estimation procedure is the linear procedure. We can show this through Pinsker bound, which we might go over in the future posts under decision-theory tag. </p>
<h2 id="Goal-of-inference-problem"><a href="#Goal-of-inference-problem" class="headerlink" title="Goal of inference problem"></a>Goal of inference problem</h2><p>Estimate $\theta$. Our loss function is </p>
<span>$$|| \hat{\theta} - \theta ||^2 = \sum_{i=1}^{\infty} (\hat{\theta}_i - \theta_i)^2$$</span><!-- Has MathJax -->
<h2 id="Estimation-Procedure"><a href="#Estimation-Procedure" class="headerlink" title="Estimation Procedure"></a>Estimation Procedure</h2><p>Intuitively, due to our parameter space restriction, when $i$ is large, $\theta_i$ has to become smaller, and at some point or later, it becomes neglible. This observation leads to the following naive estimator:</p>
<span>$$\hat{\theta}_i =  
\begin{cases} 
        Y_i &amp; i \le I  \\
        0    &amp; i &gt; I 
\end{cases}$$</span><!-- Has MathJax --> 
<p>where $I$ is the threshold value we will optimize (in terms of having a tigher risk lower bound) soon. </p>
<h2 id="Risk-Calculation"><a href="#Risk-Calculation" class="headerlink" title="Risk Calculation"></a>Risk Calculation</h2><p>With this procedure, we can easily calculate risk for each case. That is, when $\hat{\theta}_i = Y_i$, its risk is $E (Y_i - \theta_i)^2 = \frac{1}{n}$ (Note: this is just the variance of $Y_i$, which is $\frac{1}{n}$ from the definition of our model.) When $\hat{\theta}_i = 0$, its risk is $E (0 - \theta_i)^2 = \theta_i^2$ (Note that expectation is taken over data, and $\theta$ is deterministic value.) </p>
<p>With this simple algebra above, we observe that if $\theta^2_i \le \frac{1}{n}$, we want to use $\hat{\theta}_i = 0$, which is imposed by $i &gt; I$ condition. </p>
<p>The risk of this procedure is given by</p>
<span>$$R(\hat{\theta}) = E \sum_{i=1}^{\infty} (\hat{\theta}_i - \theta)^2 = E \sum_{i=1}^I (Y_i - \theta_i)^2 + E \sum_{i=I+1}^{\infty} \theta^2_i$$</span><!-- Has MathJax --> 
<h2 id="Risk-upper-bound"><a href="#Risk-upper-bound" class="headerlink" title="Risk upper bound"></a>Risk upper bound</h2><p>We immediately see that the first term is equivalent to $\frac{I}{n}$.<br>The question is how to upper bound the second term.<br>Recall the condition of our parameter space definition: $\sum_{i=0}^{\infty} i^{2 \alpha} \theta_i^2  \le M$. From this, we observe the following inequalities:</p>
<span>$$\sum_{i=0}^{\infty} i^{2 \alpha} \theta_i^2 \le M \\ 
\Rightarrow  \sum_{i = I + 1}^{\infty} i^{2 \alpha} \theta_i^2 \le M  \\ 
\Rightarrow (I+1)^{2\alpha} \sum_{i = I + 1}^{\infty} \theta_i^2 \le M \\ 
\Rightarrow \sum_{i = I + 1}^{\infty} \theta_i^2 \le \frac{M}{(I+1)^{2\alpha}} \le \frac{M}{I^{2 \alpha}}$$</span><!-- Has MathJax --> 
<p>So our upper bound is</p>
<span>$$R(\hat{\theta}) \le  \frac{I}{n} + \frac{M}{I^{2 \alpha}}$$</span><!-- Has MathJax --> 
<p>To find an optimal $I$, we will treat $\frac{M}{I^{2 \alpha}}$ as $Bias^2$, and pick $I$ to achieve the optimal variance-bias tradeoff. This leads to </p>
<span>$$\frac{I}{n} = \frac{M}{I^{2 \alpha}} \iff I^{1+2\alpha} = Mn \iff I = (Mn)^{\frac{1}{1+2\alpha}}$$</span><!-- Has MathJax --> 
<p>Therefore, </p>
<span>$$R(\hat{\theta}) \le \frac{I}{n} + \frac{M}{I^{2 \alpha}} = \frac{2}{n} (Mn)^{\frac{1}{1+2\alpha}} = 2 M^{\frac{1}{1+2\alpha}} n^{-\frac{2\alpha}{1+2\alpha}}$$</span><!-- Has MathJax --> 
<p>This leads to our risk upper bound:</p>
<span>$$sup_{\Theta} E || \hat{\theta} - \theta ||^2 \le  2 M^{\frac{1}{1+2\alpha}} n^{-\frac{2\alpha}{1+2\alpha}}$$</span><!-- Has MathJax --> 
<p>(We will try to find a better rate in some later posts.)</p>
<h2 id="Risk-lower-bound"><a href="#Risk-lower-bound" class="headerlink" title="Risk lower bound"></a>Risk lower bound</h2><p>To find lower bound, we resort to Le Cum’s idea. </p>
<p>We first construct a sub-parameter space so that the resulting risk calculation over the sub-parameter space is simple enough. Considering a sub-parameter space is useful because for any sub-parameter space $\Theta_0 \subset \Theta$, we have the following lower bound for sup risk for $\Theta$. </p>
<span>$$sup_{\Theta} E || \hat{\theta} - \theta||^2 \ge sup_{\Theta_0} E || \hat{\theta} - \theta ||^2$$</span><!-- Has MathJax --> 
<p>In our case, let our sub-parameter space be</p>
<span>$$\Theta_0 = \{ \theta : \theta_i = 0 \text{ if } i \ge I + 1, \theta_i = \frac{1}{\sqrt{n}} \text{ if } i \le I \}$$</span><!-- Has MathJax -->
<p>where $I$ is the previous optimal value : $I = (Mn)^{\frac{1}{1+2\alpha}}$. </p>
<p>We can confirm that <span>$\Theta_0$</span><!-- Has MathJax --> is indeed in <span>$\Theta$</span><!-- Has MathJax -->  by checking the condition <span>$\sum_{i=1}^{\infty} i^{2\alpha} \theta_i^2 \le M$</span><!-- Has MathJax -->: </p>
<span>$$\sum_{i=1}^I i^{2 \alpha} \frac{1}{n} = \frac{1}{n} \sum_{i=1}^I i^{2\alpha} \le \frac{1}{n} I I^{2\alpha} = \frac{1}{n} Mn = M$$</span><!-- Has MathJax --> 
<p>Now, we want to lower bound <span>$sup_{\Theta_0} E || \hat{\theta} - \theta ||^2$</span><!-- Has MathJax -->: </p>
<span>$$sup_{\Theta_0} E || \hat{\theta} - \theta ||^2 \ge  sup_{\Theta_0} E \sum_{i=1}^I \hat{\theta} - \theta ||^2  \\$$</span><!-- Has MathJax -->
<p>Now assume a prior on $\theta_i$ such that</p>
<span>$$\theta_i =
\begin{cases}
    0 &amp; \text{ w.p. } \frac{1}{2} \\
    \frac{1}{\sqrt{n}} &amp;  \text{ w.p. } \frac{1}{2} 
\end{cases}$$</span><!-- Has MathJax --> 
<p>Then, by observing that $sup_x f(x) \ge E_x f(x)$, we have </p>
<span>$$sup_{\Theta_0} E \sum_{i=1}^I || \hat{\theta} - \theta ||^2  \ge E_{\theta} E_{Y|\theta} [ \sum_{i=1}^I (\hat{\theta}_i - \theta_i )^2 ]$$</span><!-- Has MathJax -->
<p>Now recall that Bayes estimator is supposed to attain the smallest risk. This gives us the following further lower bound:</p>
<span>$$E_{\theta} E_{Y|\theta} [ \sum_{i=1}^I (\hat{\theta}_i - \theta_i )^2 ] \ge E_{\theta} E_{Y|\theta} [ \sum_{i=1}^I (\hat{\theta}_{i,Bayes} - \theta_i )^2 ]$$</span><!-- Has MathJax --> 
<p>Note that <span>$\hat{\theta}_{i,Bayes}$</span><!-- Has MathJax --> is a function of $Y_i$ only due to $i.i.d$ assumption. Therefore using our definition of prior,</p>
<span>$$E_{\theta} E_{Y|\theta} [ \sum_{i=1}^I (\hat{\theta}_{i,Bayes} - \theta_i )^2 ] 
= \sum_{i=1}^I [ \frac{1}{2} E_{Y_i|\theta_i=0} (\hat{\theta}_{i,Bayes} - 0)^2 + \frac{1}{2} E_{Y_i|\theta_i=\frac{1}{\sqrt{n}}} (\hat{\theta}_{i,Bayes} - \frac{1}{\sqrt{n}})^2 ]$$</span><!-- Has MathJax --> 
<p>Rewriting the above equation with $\phi_{a,b}(x)$ be the pdf of N(a,b) yields</p>
<span>$$\sum_{i=1}^I [ \frac{1}{2} E_{Y_i|\theta_i=0} (\hat{\theta}_{i,Bayes} - 0)^2 + \frac{1}{2} E_{Y_i|    \theta_i=\frac{1}{\sqrt{n}}} (\hat{\theta}_{i,Bayes} - \frac{1}{\sqrt{n}})^2  \\
= \sum_{i=1}^I \frac{1}{2} [ \int (\hat{\theta}_{i,Bayes} - 0)^2 \phi_{0,\frac{1}{n}}(x)dx + \int (\hat{\theta}_{i,Bayes} - \frac{1}{\sqrt{n}})^2  \phi_{\frac{1}{\sqrt{n}},\frac{1}{n}}(x) dx ]$$</span><!-- Has MathJax --> 
<p>Let <span>$g(x) = \min \{ \phi_{0,\frac{1}{n}}(x), \phi_{\frac{1}{\sqrt{n}},\frac{1}{n}}(x) \}$</span><!-- Has MathJax -->. Then using $g(x)$, the above equation can be lower bounded:</p>
<span>$$\ge \sum_{i=1}^I \frac{1}{2} \int \left( ( \hat{\theta}_{i,Bayes} - 0)^2 + (\hat{\theta}_{i,Bayes} - \frac{1}{\sqrt{n}})^2 \right)  g(x) dx$$</span><!-- Has MathJax -->
<p>Observing that <span>$a^2 + b^2 \ge \frac{1}{2} (a-b)^2$</span><!-- Has MathJax -->, we can rewrite it to</p>
<span>$$\ge \sum_{i=1}^I \frac{1}{2} \int \frac{1}{2} (\frac{1}{\sqrt{n}})^2 g(x) dx = \frac{I}{n} \frac{1}{4} \int g(x) dx$$</span><!-- Has MathJax -->
<p>Since $\int g(x) dx$ is some constant $c$, our risk lower bound is</p>
<span>$$c \frac{I}{n} = c n^{- \frac{2 \alpha}{2 \alpha + 1}}$$</span><!-- Has MathJax --> 
<p>This establishes our risk lower bound for this particular estimation procedure under this model. </p>

      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/blog/tags/decision-theory/">decision-theory</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Kommentare</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="http://runopti.github.io/blog/2017/03/12/nonparametric-estimation/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Suche">
    <input type="hidden" name="q" value="site:runopti.github.io/blog">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/blog/tags/PRML/">PRML</a><small>1</small></li>
  
    <li><a href="/blog/tags/cv/">cv</a><small>1</small></li>
  
    <li><a href="/blog/tags/decision-theory/">decision-theory</a><small>2</small></li>
  
    <li><a href="/blog/tags/math/">math</a><small>7</small></li>
  
    <li><a href="/blog/tags/neuralnet/">neuralnet</a><small>8</small></li>
  
    <li><a href="/blog/tags/optimization/">optimization</a><small>5</small></li>
  
    <li><a href="/blog/tags/paper-memo/">paper-memo</a><small>2</small></li>
  
    <li><a href="/blog/tags/research/">research</a><small>0</small></li>
  
    <li><a href="/blog/tags/variational-inference/">variational-inference</a><small>1</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2017 Yutaro Yamada
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/blog/js/jquery.imagesloaded.min.js"></script>
<script src="/blog/js/gallery.js"></script>




<link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/blog/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]],"processEscapes":true}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
