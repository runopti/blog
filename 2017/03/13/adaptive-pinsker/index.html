<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Nonparametric Regression 02 | Notes</title>
  <meta name="author" content="Yutaro Yamada">
  
  <meta name="description" content="In the last post, we investigated a Gaussian Sequence model and showed minimax rate:
$$\inf_{\hat{\theta}} \sup_{\Theta} E || \hat{\theta} - \theta ||">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Nonparametric Regression 02"/>
  <meta property="og:site_name" content="Notes"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/blog/favicon.png" rel="icon">
  <link rel="alternate" href="/blog/atom.xml" title="Notes" type="application/atom+xml">
  <link rel="stylesheet" href="/blog/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/blog/">Notes</a></h1>
  <h2><a href="/blog/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/blog/">Home</a></li>
    
      <li><a href="/blog/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-13T16:20:10.000Z"><a href="/blog/2017/03/13/adaptive-pinsker/">2017-03-13</a></time>
      
      
  
    <h1 class="title">Nonparametric Regression 02</h1>
  

    </header>
    <div class="entry">
      
        <p>In the last post, we investigated a Gaussian Sequence model and showed minimax rate:</p>
<span>$$\inf_{\hat{\theta}} \sup_{\Theta} E || \hat{\theta} - \theta ||^2 \asymp M^{\frac{1}{2\alpha + 1}} n^{-\frac{2\alpha}{2\alpha+1}}$$</span><!-- Has MathJax --> 
<p>The crucial assumption to derive this bound is that we assumed $\alpha$ and $M$ are known, which defines a Sobolev ellipsoid, our parameter space. </p>
<p>In this post, we remove that assumption and attempt to achieve the same risk bound (up to some constant).</p>
<p>To recap, the followings are our model description and parameter space.</p>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><span>$$Y_i = \theta_i + \frac{1}{\sqrt{n}} \sigma Z_i, \text{ where } Z_i \sim^{iid} N(0,1)$$</span><!-- Has MathJax --> 
<h3 id="Parameter-space"><a href="#Parameter-space" class="headerlink" title="Parameter space"></a>Parameter space</h3><span>$$\Theta = \{ \theta : \sum_{i=1}^{\infty} i^{2 \alpha} \theta^2_i \le M \}$$</span><!-- Has MathJax -->
<h2 id="Adaptive-Estimation"><a href="#Adaptive-Estimation" class="headerlink" title="Adaptive Estimation"></a>Adaptive Estimation</h2><p>Recall that the naive estimation procedure in the previous post just uses an observation as our estimator for small $i$s and 0 for large $i$s.<br>It worked because we were able to optimize $I$ based on $\alpha$ and $M$.<br>Now since we don’t have access to $\alpha$ and $M$, we have to take an alternative estimation procedure.<br>Since the only available information we have is data, we want our procedure to reflect this information in a meaningful way.  </p>
<p>To get a hint, we will look into how James-Stein estimator was conceived.</p>
<h3 id="James-Stein-Estimator"><a href="#James-Stein-Estimator" class="headerlink" title="James-Stein Estimator"></a>James-Stein Estimator</h3><p>James-Stein estimator is a form of $\delta(X) = a^T X$ (linear procedure) where $a$ is a data-dependent parameter.<br>(That is, JS estimator is adaptive to the $|| \theta ||^2$.) </p>
<p>If we have $m$ observations such that</p>
<span>$$X_i = \theta_i + \sigma Z_i , Z_i \sim^{i.i.d} N(0,1)$$</span><!-- Has MathJax --> 
<p>Then JS estimator is</p>
<span>$$\hat{\theta}_{JS} = (1 - \frac{(m-2)\sigma^2}{\sum_{i=1}^m X_i^2} ) X$$</span><!-- Has MathJax --> 
<p>where $X$ represents a $m$ dimensional vector (so we put all the $m$ observations into one vector). </p>
<p>We say that JS-estimator is mimicking linear estimator because the difference of the two risks is bounded by some constant:</p>
<span>$$E || \hat{\theta}_{JS} - \theta ||^2 - \inf_c E || c X - \theta ||^2 \le \text{constant}$$</span><!-- Has MathJax --> 
<p>To see this, we will investigate each term and show the difference is bounded.<br>(assume $\sigma=1$ without loss of generality.) </p>
<h3 id="The-First-Term"><a href="#The-First-Term" class="headerlink" title="The First Term"></a>The First Term</h3><span>$$E || \hat{\theta}_{JS} - \theta ||^2 =  E_{X|\theta} \sum_{i=1}^m (X_i - \frac{m-2}{||X||^2} - \theta_i)^2  \\
= E_{X|\theta} \sum_{i=1}^m ((X_i - \theta)^2 + \frac{(m-2)^2}{(||X||^2)^2} X_i^2 - 2(X_i - \theta_i) \frac{m-2}{||X||^2} X_i  ) \\
= m + E_{X|\theta} (\frac{(m-2)^2}{||X||^2} ) - 2 \sum_{i=1}^m E_{X|\theta} (X_i - \theta) \frac{m-2}{||X||^2} X_i$$</span><!-- Has MathJax -->
<p>To go further, we will need Stein’s identity (simplified version):</p>
<span>$$\text{For } Y \sim N(\theta, 1), \text{ we have } 
E_{Y|\theta} (Y - \theta) g(Y) = E_{Y|\theta} g&apos;(Y)$$</span><!-- Has MathJax --> 
<p>under some regularity assumption for $g$. </p>
<p>In our case, $Y=X_i$ and  $g(X_i) = \frac{m-2}{||X||^2} X_i$.</p>
<p>By the above identity, we have</p>
<span>$$\sum_{i=1}^m E_{X|\theta} (X_i - \theta) \frac{m-2}{||X||^2} X_i
= \sum_{i=1}^m E_{X|\theta} \frac{\partial}{\partial X_i}(\frac{m-2}{||X||^2}X_i) \\
= \sum_{i=1}^m E_{X|\theta}( \frac{m-2}{||X||^2} + \frac{-(m-2) 2X_i}{(\sum_{i=1}^m X_i^2)^2}X_i ) \\
= E_{X|\theta} (\frac{m(m-2)}{||X||^2} - \frac{2 (m-2)}{||X||^2} ) \\
= E_{X|\theta} (\frac{m(m-2)^2}{||X||^2}  ) \\$$</span><!-- Has MathJax --> 
<p>This gives us</p>
<span>$$E || \hat{\theta}_{JS} - \theta ||^2 =  m + E_{X|\theta} (\frac{(m-2)^2}{||X||^2} ) - 2 \sum_{i=1}^m E_{X|\theta} (X_i - \theta) \frac{m-2}{||X||^2} X_i \\
= m - E_{X|\theta} \frac{(m-2)^2}{||X||^2}$$</span><!-- Has MathJax --> 
<p>Now to get an upper bound on this, we will apply Jensen’s inequality but before that we will use the following property of noncentral Chi-squared distribution to simplify (noncentral because each $X_i$ has a different mean $\theta_i$):</p>
<span>$$||X||^2 = \sum_{i=1}^m X_i^2 \sim \chi_{m+2N},&nbsp;\text{ where } N \sim Poisson \left(\frac{||\theta||^2}{2} \right)$$</span><!-- Has MathJax --> 
<p>By conditioning on $N$, we have</p>
<span>$$m - E_{X|\theta} \frac{(m-2)^2}{||X||^2} = m - E_N \left[ E_{X|N} \frac{(m-2)^2}{||X||^2} | N \right]  \\
= m - E_N \frac{(m-2)^2}{m+2N-2}  \text{  (Note that } E\frac{1}{\chi^2_d} = \frac{1}{d-2} \text{)} \\
\le m - \frac{(m-2)^2}{m-2 + 2E_N[n]} \text{  Note (Jensen&apos;s inequality):} E \frac{1}{Y} \ge \frac{1}{EY} \\
= m - \frac{(m-2)^2}{m-2 + ||\theta||^2} \\
= 2 + m - 2 - \frac{(m-2)^2}{m-2 + ||\theta||^2} \\
= 2 + \frac{(m-2)||\theta||^2}{m-2 + ||\theta||^2} \\
\le 2 + \frac{m||\theta||^2}{m + ||\theta||^2}$$</span><!-- Has MathJax -->
<p>When $\sigma \neq 1$, it’s going to be </p>
<span>$$E ||\hat{\theta} - \theta||^2  \le 2\sigma^2 + \frac{m \sigma^2 ||\theta||^2}{m\sigma^2 + ||\theta||^2}$$</span><!-- Has MathJax -->
<h3 id="The-Second-Term"><a href="#The-Second-Term" class="headerlink" title="The Second Term"></a>The Second Term</h3><p>We can see that<br><span>$$\inf_c E || c X - \theta ||^2 = \frac{m \sigma^2 ||\theta||^2}{||\theta||^2 + m \sigma^2}$$</span><!-- Has MathJax --></p>
<p>by optimizing $c$ to get $inf$. </p>
<p>First, consider the bias-variance decomposition for squared loss:</p>
<span>$$E || cX - \theta ||^2 = (c-1)^2 ||\theta||^2 + c^2 m \sigma^2$$</span><!-- Has MathJax -->
<p>By taking the derivative w.r.t. $c$ and setting it equal to zero, we have</p>
<span>$$2(c-1) ||\theta||^2 + 2cm\sigma^2 = 0 \\
c = \frac{||\theta||^2}{||\theta||^2 + m \sigma^2}$$</span><!-- Has MathJax -->
<p>By plugging this value into  $E||cX - \theta||^2$, we have</p>
<span>$$(c-1)^2 ||\theta||^2 + c^2 m \sigma^2 = (\frac{-m\sigma^2}{||\theta||^2 + m \sigma^2})^2 ||\theta||^2 + (\frac{||\theta||^2}{||\theta||^2 + m \sigma^2})^2 m \sigma^2 \\
= \frac{m \sigma^2 ||\theta||^2 }{||\theta||^2 + m \sigma^2 }$$</span><!-- Has MathJax --> 
<p>By combining the results from First Term and Second Term, we have</p>
<span>$$E || \hat{\theta}_{JS} - \theta ||^2 - \inf_c E || c X - \theta ||^2 \le 2 \sigma^2$$</span><!-- Has MathJax --> 
<p>which shows that the risk difference is bounded by $2 \sigma^2$, a constant that doesn’t depend on $c$. </p>
<h2 id="Shrinkage-Estimator"><a href="#Shrinkage-Estimator" class="headerlink" title="Shrinkage Estimator"></a>Shrinkage Estimator</h2><p>Now let’s go back to our original problem of constructing an estimation procedure that achieves </p>
<span>$$E || \hat{\theta} - \theta ||^2 \asymp M^\frac{1}{1+2 \alpha} n^{- \frac{2 \alpha}{2 \alpha+1}}$$</span><!-- Has MathJax --> 
<p>By leveraging the idea of James-Stein estimator, we can think of a form of shrinkage estimator for this problem as well.<br>However, simply using $\hat{\theta} = \hat{\theta}_{JS}$ doesn’t work, because JS implicitely assumes that $\theta_i$ ($i=1,…,m$) are all in the same magnitude, which means that when some $\theta_i$ is big and others are small, the performance will be really bad.<br>This makes sense because we equally shrink $X$ by the same factor: $(1 - \frac{c}{||X||^2})$</p>
<p>The natural extention of the JS procedure is to divide the observation into many blocks and apply a different shrinkcage factor for each block, according to its magnitude. (WLOG, we can assume our observations are sorted in a decreasing order.)</p>
<h2 id="Risk-Calculation"><a href="#Risk-Calculation" class="headerlink" title="Risk Calculation"></a>Risk Calculation</h2><p>The modified JS-like estimation procedure leads us to the following risk:</p>
<span>$$E || \hat{\theta} - \theta ||^2 = E \sum_{i=1}^n (\hat{\theta}_i - \theta_i)^2 + \sum_{i=n+1}^{\infty} \theta_i^2  \\

\le \sum_{k=1}^K \left[ \frac{|B_k| \frac{1}{n} ||\theta_{B_k}||^2}{|B_k|\frac{1}{n}+||\theta_{B_k}||^2} + 2 \frac{1}{n} \right] + O(n^{-\frac{2\alpha}{2\alpha+1}})$$</span><!-- Has MathJax --> 
<p>where $K$ is the number of blocks, $B_k$ represents the $k$ th block, $|B_k|$ is the size of that block.<br>We can naively set the number of elements in each block to be the power of 2, which allows us to write $K = \log_2 n$, where $n$ is the number of observations in total. </p>
<p>The correspondence from the James-Stein estimator risk upper bound is: </p>
<span>$$\sigma^2 = \frac{1}{n}, ||\theta||^2 = ||\theta_{B_k}||^2, m = |B_k|$$</span><!-- Has MathJax -->
<p>Recall that we introduced the threshold index $I$ by considering bias-variance trade-off.<br>This time, we will try to find $K_0$ (the number of blocks) such that $I \le |B<em>1| + \cdots |B</em>{K_0}| \le 2I$ holds. </p>
<p>Now observe that for any positive real number $a$ and $b$, we have $\frac{ab}{a+b} \le a$ and $\frac{ab}{a+b} \le b$.<br>Therefore,</p>
<span>$$\sum_{k=1}^K \left[ \frac{|B_k| \frac{1}{n} ||\theta_{B_k}||^2}{|B_k|\frac{1}{n}+||\theta_{B_k}||^2} + 2 \frac{1}{n} \right] + O(n^{-\frac{2\alpha}{2\alpha+1}}) \\
\le \sum_{k=1}^{K_0} |B_k|\frac{1}{n} + \sum_{k=K_0+1}^K ||\theta_{B_k}||^2 + 2 \frac{1}{n} K + O(n^{-\frac{2 \alpha}{2\alpha+1}}) \\
\le \frac{1}{n} 2I + \sum_{i &gt; I}^{\infty} \theta_i^2 + + O(n^{-\frac{2 \alpha}{2\alpha+1}}) \\$$</span><!-- Has MathJax -->
<p>From the calculation in the last post, we see that this will be upper bounded by</p>
<span>$$\le c M^{\frac{1}{1+2\alpha}} n^{-\frac{2\alpha}{2\alpha+1}}$$</span><!-- Has MathJax -->
<p>One criticism toward this procedure is that the constant $c$ could be huge.<br>Is there a way to reduce it? We will explore alternative blockwise procedure next time.</p>

      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/blog/tags/decision-theory/">decision-theory</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Kommentare</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="http://runopti.github.io/blog/2017/03/13/adaptive-pinsker/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Suche">
    <input type="hidden" name="q" value="site:runopti.github.io/blog">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/blog/tags/PRML/">PRML</a><small>1</small></li>
  
    <li><a href="/blog/tags/cv/">cv</a><small>1</small></li>
  
    <li><a href="/blog/tags/decision-theory/">decision-theory</a><small>3</small></li>
  
    <li><a href="/blog/tags/math/">math</a><small>7</small></li>
  
    <li><a href="/blog/tags/neuralnet/">neuralnet</a><small>9</small></li>
  
    <li><a href="/blog/tags/optimization/">optimization</a><small>5</small></li>
  
    <li><a href="/blog/tags/paper-memo/">paper-memo</a><small>2</small></li>
  
    <li><a href="/blog/tags/research/">research</a><small>0</small></li>
  
    <li><a href="/blog/tags/variational-inference/">variational-inference</a><small>2</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2017 Yutaro Yamada
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/blog/js/jquery.imagesloaded.min.js"></script>
<script src="/blog/js/gallery.js"></script>




<link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/blog/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]],"processEscapes":true}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
