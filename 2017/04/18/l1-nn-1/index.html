<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>l1-nn-1 | Notes</title>
  <meta name="author" content="Yutaro Yamada">
  
  <meta name="description" content="Notes for l1-legularized Neural Networks are Improperly Learnable in Polynomial Time
Problem SettingModel:For $|| x ||_2 \le 1$, the class of function">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="l1-nn-1"/>
  <meta property="og:site_name" content="Notes"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/blog/favicon.png" rel="icon">
  <link rel="alternate" href="/blog/atom.xml" title="Notes" type="application/atom+xml">
  <link rel="stylesheet" href="/blog/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/blog/">Notes</a></h1>
  <h2><a href="/blog/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/blog/">Home</a></li>
    
      <li><a href="/blog/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-04-19T00:49:09.000Z"><a href="/blog/2017/04/18/l1-nn-1/">2017-04-18</a></time>
      
      
  
    <h1 class="title">l1-nn-1</h1>
  

    </header>
    <div class="entry">
      
        <p>Notes for <a href="https://arxiv.org/pdf/1510.03528.pdf" target="_blank" rel="external">l1-legularized Neural Networks are Improperly Learnable in Polynomial Time</a></p>
<h2 id="Problem-Setting"><a href="#Problem-Setting" class="headerlink" title="Problem Setting"></a>Problem Setting</h2><h3 id="Model"><a href="#Model" class="headerlink" title="Model:"></a>Model:</h3><p>For $|| x ||_2 \le 1$, the class of functions we consider is</p>
<span>$\mathcal{N}_{k,L,\sigma} =  \left\{ f(x) = \sigma ( \cdots (\sigma(x\mathbf{W^{(1)}}) \cdots \mathbf{W^{(k)}} ) , \left\lVert \mathbf{W}^{(0)}_i \right\rVert_2 \le L, \left\lVert \mathbf{W}^{(l)}_i \right\rVert_1 \le L  \text{ for } 1 \le l \le k \right\}$</span><!-- Has MathJax --> 
<p>where $\mathbf{W}_i$ represents its $i$th row vector. </p>
<h3 id="Assumption-in-words"><a href="#Assumption-in-words" class="headerlink" title="Assumption (in words):"></a>Assumption (in words):</h3><ul>
<li>The input vector $x$ has bounded $\ell_2$ norm: $\left\lVert x \right\rVert \le 1$</li>
<li><p>The edge weights in the first layer have bounded $\ell_2$ norms.<br>($\rightarrow$ each row of $\mathbf{W^{(1)}}$ is bounded)</p>
</li>
<li><p>The edge weights in the other layers have bounded $\ell_1$ norms.<br>($\rightarrow$ The $\ell_1$-regularization induces sparsity on the neural network.)</p>
</li>
</ul>
<h3 id="Goal"><a href="#Goal" class="headerlink" title="Goal:"></a>Goal:</h3><p>Learn a predictor $f: \mathcal{X} \rightarrow \mathbb{R}$ whose generalization loss is at most $\epsilon$ worse than that  of the best neural network in $\mathcal{N}_{k,L,\sigma}$</p>
<h2 id="What’s-shown-in-the-paper"><a href="#What’s-shown-in-the-paper" class="headerlink" title="What’s shown in the paper:"></a>What’s shown in the paper:</h2><h4 id="Claim"><a href="#Claim" class="headerlink" title="Claim:"></a>Claim:</h4><p>Theorem 1 implies that the neural network activated by <span>$\sigma_{erf}$</span><!-- Has MathJax --> or <span>$\sigma_{sh}$</span><!-- Has MathJax --> is learnable in polynomial time given any constant $(k, L)$. </p>
<h4 id="Proof-Steps"><a href="#Proof-Steps" class="headerlink" title="Proof Steps:"></a>Proof Steps:</h4><ul>
<li><p>First show that the RKHS <span>$F_{k,B}$</span><!-- Has MathJax --> (induced by the kernel $K^k(x,y)$ which I will describe shortly) contains <span>$\mathcal{N_{k,L,\sigma}}$</span><!-- Has MathJax --> by arguing that any input to a neuron (which is a function of $x \in \mathcal{X}$) is an element of RKHS $F_{k,B}$. </p>
</li>
<li><p>Then show that if we use <span>$\sigma_{erf}$</span><!-- Has MathJax --> or <span>$\sigma_{sh}$</span><!-- Has MathJax -->, the norm of an input of an neuron in any layer will be bounded (which is equivalent to saying that $||f|| \le B$ for all $f \in F_{k,B}$.) (This part corresponds to Proposition 1 in the paper.)</p>
</li>
<li><p>Invoke Theorem 2.2 from <a href="https://www.cs.cornell.edu/~sridharan/sicomp.pdf" target="_blank" rel="external">(Shalev-Shwartz, 2011)</a> to establish the claim. </p>
</li>
</ul>
<p>To be more precise in Step 2, they first construct this quantity $H(\lambda)$, which gives the upper bound on the norm of an input of any neuron. (This is shown in the proof for Lemma 1 in the Appendix A.) They then show that $H(\lambda)$ is bounded if we use <span>$\sigma_{erf}$</span><!-- Has MathJax --> or <span>$\sigma_{sh}$</span><!-- Has MathJax -->. </p>
<h3 id="Kernel-function"><a href="#Kernel-function" class="headerlink" title="Kernel function"></a>Kernel function</h3><p>The kernel function they used is the following:<br><span>$$K(x,y) := \frac{1}{2 - &lt;x,y&gt;}$$</span><!-- Has MathJax --> </p>
<p>The validity of this kernel is proven by explicitly constructing a mapping $\psi(x)$ s.t. $k(x,y) = &lt;\psi(x), \psi(y)&gt;$.<br>That is, they consider $\psi(x)$ as an infinite dimensional vector, indexed by $i$, where $i = (k_1, … , k_j)$ for all <span>$(k_1,...,k_j) \in \{1,...,n\}$</span><!-- Has MathJax --> and $j=1,..,\infty$. (So if we treat $\psi(x)$ as a $M-$dimensional vector, there exsits <span>$\sum_{j=0}^M n^j$</span><!-- Has MathJax --> number of entries in $\psi(x)$. Of course, this is just to grasp the picture of this vector. In reality, $M$ is infinity.)</p>
<h2 id="Recursive-Kernel-Method"><a href="#Recursive-Kernel-Method" class="headerlink" title="Recursive Kernel Method"></a>Recursive Kernel Method</h2>
      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/blog/tags/neuralnet/">neuralnet</a>, <a href="/blog/tags/learning-theory/">learning-theory</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Kommentare</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="http://runopti.github.io/blog/2017/04/18/l1-nn-1/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Suche">
    <input type="hidden" name="q" value="site:runopti.github.io/blog">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/blog/tags/PRML/">PRML</a><small>1</small></li>
  
    <li><a href="/blog/tags/cv/">cv</a><small>1</small></li>
  
    <li><a href="/blog/tags/decision-theory/">decision-theory</a><small>3</small></li>
  
    <li><a href="/blog/tags/learning-theory/">learning-theory</a><small>3</small></li>
  
    <li><a href="/blog/tags/math/">math</a><small>7</small></li>
  
    <li><a href="/blog/tags/neuralnet/">neuralnet</a><small>10</small></li>
  
    <li><a href="/blog/tags/optimization/">optimization</a><small>5</small></li>
  
    <li><a href="/blog/tags/paper-memo/">paper-memo</a><small>2</small></li>
  
    <li><a href="/blog/tags/research/">research</a><small>0</small></li>
  
    <li><a href="/blog/tags/variational-inference/">variational-inference</a><small>2</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2017 Yutaro Yamada
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/blog/js/jquery.imagesloaded.min.js"></script>
<script src="/blog/js/gallery.js"></script>




<link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/blog/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]],"processEscapes":true}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
