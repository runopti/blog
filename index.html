<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Notes</title>
  <meta name="author" content="Yutaro Yamada">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Notes"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/blog/favicon.png" rel="icon">
  <link rel="alternate" href="/blog/atom.xml" title="Notes" type="application/atom+xml">
  <link rel="stylesheet" href="/blog/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/blog/">Notes</a></h1>
  <h2><a href="/blog/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/blog/">Home</a></li>
    
      <li><a href="/blog/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-15T16:06:06.000Z"><a href="/blog/2017/03/15/alternative-blockwise/">2017-03-15</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2017/03/15/alternative-blockwise/">Nonparametric Regression 03</a></h1>
  

    </header>
    <div class="entry">
      
        <p>In this post, we will modify the blockwise estimation procedure we discussed last time, to reduce the constant factor appeared in our upper bound on the expected risk. </p>
<h2 id="Alternative-Blockwise-Estimator"><a href="#Alternative-Blockwise-Estimator" class="headerlink" title="Alternative Blockwise Estimator"></a>Alternative Blockwise Estimator</h2><p>The issue is that the number of blocks could be very small compared to the number of observations. For example, when $n=1024$, the number of blocks $K = 10$. </p>
<p>Previously, we set each block size to be the power of 2. That is, $|B_k| = 2^k$. </p>
<p>To have a smaller blocksize, we can set $|B_k| = \lfloor (1+a)^k \rfloor$, where $0 &lt; a &lt; 1$. </p>
<p>(WIP)</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-13T16:20:10.000Z"><a href="/blog/2017/03/13/adaptive-pinsker/">2017-03-13</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2017/03/13/adaptive-pinsker/">Nonparametric Regression 02</a></h1>
  

    </header>
    <div class="entry">
      
        <p>In the last post, we investigated a Gaussian Sequence model and showed minimax rate:</p>
<span>$$\inf_{\hat{\theta}} \sup_{\Theta} E || \hat{\theta} - \theta ||^2 \asymp M^{\frac{1}{2\alpha + 1}} n^{-\frac{2\alpha}{2\alpha+1}}$$</span><!-- Has MathJax --> 
<p>The crucial assumption to derive this bound is that we assumed $\alpha$ and $M$ are known, which defines a Sobolev ellipsoid, our parameter space. </p>
<p>In this post, we remove that assumption and attempt to achieve the same risk bound (up to some constant).</p>
<p>To recap, the followings are our model description and parameter space.</p>
<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><span>$$Y_i = \theta_i + \frac{1}{\sqrt{n}} \sigma Z_i, \text{ where } Z_i \sim^{iid} N(0,1)$$</span><!-- Has MathJax --> 
<h3 id="Parameter-space"><a href="#Parameter-space" class="headerlink" title="Parameter space"></a>Parameter space</h3><span>$$\Theta = \{ \theta : \sum_{i=1}^{\infty} i^{2 \alpha} \theta^2_i \le M \}$$</span><!-- Has MathJax -->
<h2 id="Adaptive-Estimation"><a href="#Adaptive-Estimation" class="headerlink" title="Adaptive Estimation"></a>Adaptive Estimation</h2><p>Recall that the naive estimation procedure in the previous post just uses an observation as our estimator for small $i$s and 0 for large $i$s.<br>It worked because we were able to optimize $I$ based on $\alpha$ and $M$.<br>Now since we don’t have access to $\alpha$ and $M$, we have to take an alternative estimation procedure.<br>Since the only available information we have is data, we want our procedure to reflect this information in a meaningful way.  </p>
<p>To get a hint, we will look into how James-Stein estimator was conceived.</p>
<h3 id="James-Stein-Estimator"><a href="#James-Stein-Estimator" class="headerlink" title="James-Stein Estimator"></a>James-Stein Estimator</h3><p>James-Stein estimator is a form of $\delta(X) = a^T X$ (linear procedure) where $a$ is a data-dependent parameter.<br>(That is, JS estimator is adaptive to the $|| \theta ||^2$.) </p>
<p>If we have $m$ observations such that</p>
<span>$$X_i = \theta_i + \sigma Z_i , Z_i \sim^{i.i.d} N(0,1)$$</span><!-- Has MathJax --> 
<p>Then JS estimator is</p>
<span>$$\hat{\theta}_{JS} = (1 - \frac{(m-2)\sigma^2}{\sum_{i=1}^m X_i^2} ) X$$</span><!-- Has MathJax --> 
<p>where $X$ represents a $m$ dimensional vector (so we put all the $m$ observations into one vector). </p>
<p>We say that JS-estimator is mimicking linear estimator because the difference of the two risks is bounded by some constant:</p>
<span>$$E || \hat{\theta}_{JS} - \theta ||^2 - \inf_c E || c X - \theta ||^2 \le \text{constant}$$</span><!-- Has MathJax --> 
<p>To see this, we will investigate each term and show the difference is bounded.<br>(assume $\sigma=1$ without loss of generality.) </p>
<h3 id="The-First-Term"><a href="#The-First-Term" class="headerlink" title="The First Term"></a>The First Term</h3><span>$$E || \hat{\theta}_{JS} - \theta ||^2 =  E_{X|\theta} \sum_{i=1}^m (X_i - \frac{m-2}{||X||^2} - \theta_i)^2  \\
= E_{X|\theta} \sum_{i=1}^m ((X_i - \theta)^2 + \frac{(m-2)^2}{(||X||^2)^2} X_i^2 - 2(X_i - \theta_i) \frac{m-2}{||X||^2} X_i  ) \\
= m + E_{X|\theta} (\frac{(m-2)^2}{||X||^2} ) - 2 \sum_{i=1}^m E_{X|\theta} (X_i - \theta) \frac{m-2}{||X||^2} X_i$$</span><!-- Has MathJax -->
<p>To go further, we will need Stein’s identity (simplified version):</p>
<span>$$\text{For } Y \sim N(\theta, 1), \text{ we have } 
E_{Y|\theta} (Y - \theta) g(Y) = E_{Y|\theta} g&apos;(Y)$$</span><!-- Has MathJax --> 
<p>under some regularity assumption for $g$. </p>
<p>In our case, $Y=X_i$ and  $g(X_i) = \frac{m-2}{||X||^2} X_i$.</p>
<p>By the above identity, we have</p>
<span>$$\sum_{i=1}^m E_{X|\theta} (X_i - \theta) \frac{m-2}{||X||^2} X_i
= \sum_{i=1}^m E_{X|\theta} \frac{\partial}{\partial X_i}(\frac{m-2}{||X||^2}X_i) \\
= \sum_{i=1}^m E_{X|\theta}( \frac{m-2}{||X||^2} + \frac{-(m-2) 2X_i}{(\sum_{i=1}^m X_i^2)^2}X_i ) \\
= E_{X|\theta} (\frac{m(m-2)}{||X||^2} - \frac{2 (m-2)}{||X||^2} ) \\
= E_{X|\theta} (\frac{m(m-2)^2}{||X||^2}  ) \\$$</span><!-- Has MathJax --> 
<p>This gives us</p>
<span>$$E || \hat{\theta}_{JS} - \theta ||^2 =  m + E_{X|\theta} (\frac{(m-2)^2}{||X||^2} ) - 2 \sum_{i=1}^m E_{X|\theta} (X_i - \theta) \frac{m-2}{||X||^2} X_i \\
= m - E_{X|\theta} \frac{(m-2)^2}{||X||^2}$$</span><!-- Has MathJax --> 
<p>Now to get an upper bound on this, we will apply Jensen’s inequality but before that we will use the following property of noncentral Chi-squared distribution to simplify (noncentral because each $X_i$ has a different mean $\theta$):</p>
<span>$$||X||^2 = \sum_{i=1}^m X_i^2 \sim \chi_{m+2N},&nbsp;\text{ where } N \sim Poisson \left(\frac{||\theta||^2}{2} \right)$$</span><!-- Has MathJax --> 
<p>By conditioning on $N$, we have</p>
<span>$$m - E_{X|\theta} \frac{(m-2)^2}{||X||^2} = m - E_N \left[ E_{X|N} \frac{(m-2)^2}{||X||^2} | N \right]  \\
= m - E_N \frac{(m-2)^2}{m+2N-2}  \text{  (Note that } E\frac{1}{\chi^2_d} = \frac{1}{d-2} \text{)} \\
\le m - \frac{(m-2)^2}{m-2 + 2E_N[n]} \text{  Note (Jensen&apos;s inequality):} E \frac{1}{Y} \ge \frac{1}{EY} \\
= m - \frac{(m-2)^2}{m-2 + ||\theta||^2} \\
= 2 + m - 2 - \frac{(m-2)^2}{m-2 + ||\theta||^2} \\
= 2 + \frac{(m-2)||\theta||^2}{m-2 + ||\theta||^2} \\
\le 2 + \frac{m||\theta||^2}{m + ||\theta||^2}$$</span><!-- Has MathJax -->
<p>When $\sigma \neq 1$, it’s going to be </p>
<span>$$E ||\hat{\theta} - \theta||^2  \le 2\sigma^2 + \frac{m \sigma^2 ||\theta||^2}{m\sigma^2 + ||\theta||^2}$$</span><!-- Has MathJax -->
<h3 id="The-Second-Term"><a href="#The-Second-Term" class="headerlink" title="The Second Term"></a>The Second Term</h3><p>We can see that<br><span>$$\inf_c E || c X - \theta ||^2 = \frac{m \sigma^2 ||\theta||^2}{||\theta||^2 + m \sigma^2}$$</span><!-- Has MathJax --></p>
<p>by optimizing $c$ to get $inf$. </p>
<p>First, consider the bias-variance decomposition for squared loss:</p>
<span>$$E || cX - \theta ||^2 = (c-1)^2 ||\theta||^2 + c^2 m \sigma^2$$</span><!-- Has MathJax -->
<p>By taking the derivative w.r.t. $c$ and setting it equal to zero, we have</p>
<span>$$2(c-1) ||\theta||^2 + 2cm\sigma^2 = 0 \\
c = \frac{||\theta||^2}{||\theta||^2 + m \sigma^2}$$</span><!-- Has MathJax -->
<p>By plugging this value into  $E||cX - \theta||^2$, we have</p>
<span>$$(c-1)^2 ||\theta||^2 + c^2 m \sigma^2 = (\frac{-m\sigma^2}{||\theta||^2 + m \sigma^2})^2 ||\theta||^2 + (\frac{||\theta||^2}{||\theta||^2 + m \sigma^2})^2 m \sigma^2 \\
= \frac{m \sigma^2 ||\theta||^2 }{||\theta||^2 + m \sigma^2 }$$</span><!-- Has MathJax --> 
<p>By combining the results from First Term and Second Term, we have</p>
<span>$$E || \hat{\theta}_{JS} - \theta ||^2 - \inf_c E || c X - \theta ||^2 \le 2 \sigma^2$$</span><!-- Has MathJax --> 
<p>which shows that the risk difference is bounded by $2 \sigma^2$, a constant that doesn’t depend on $c$. </p>
<h2 id="Shrinkage-Estimator"><a href="#Shrinkage-Estimator" class="headerlink" title="Shrinkage Estimator"></a>Shrinkage Estimator</h2><p>Now let’s go back to our original problem of constructing an estimation procedure that achieves </p>
<span>$$E || \hat{\theta} - \theta ||^2 \asymp M^\frac{1}{1+2 \alpha} n^{- \frac{2 \alpha}{2 \alpha+1}}$$</span><!-- Has MathJax --> 
<p>By leveraging the idea of James-Stein estimator, we can think of a form of shrinkage estimator for this problem as well.<br>However, simply using $\hat{\theta} = \hat{\theta}_{JS}$ doesn’t work, because JS implicitely assumes that $\theta_i$ ($i=1,…,m$) are all in the same magnitude, which means that when some $\theta_i$ is big and others are small, the performance will be really bad.<br>This makes sense because we equally shrink $X$ by the same factor: $(1 - \frac{c}{||X||^2})$</p>
<p>The natural extention of the JS procedure is to divide the observation into many blocks and apply a different shrinkcage factor for each block, according to its magnitude. (WLOG, we can assume our observations are sorted in a decreasing order.)</p>
<h2 id="Risk-Calculation"><a href="#Risk-Calculation" class="headerlink" title="Risk Calculation"></a>Risk Calculation</h2><p>The modified JS-like estimation procedure leads us to the following risk:</p>
<span>$$E || \hat{\theta} - \theta ||^2 = E \sum_{i=1}^n (\hat{\theta}_i - \theta_i)^2 + \sum_{i=n+1}^{\infty} \theta_i^2  \\

\le \sum_{k=1}^K \left[ \frac{|B_k| \frac{1}{n} ||\theta_{B_k}||^2}{|B_k|\frac{1}{n}+||\theta_{B_k}||^2} + 2 \frac{1}{n} \right] + O(n^{-\frac{2\alpha}{2\alpha+1}})$$</span><!-- Has MathJax --> 
<p>where $K$ is the number of blocks, $B_k$ represents the $k$ th block, $|B_k|$ is the size of that block.<br>We can naively set the number of elements in each block to be the power of 2, which allows us to write $K = \log_2 n$, where $n$ is the number of observations in total. </p>
<p>The correspondence from the James-Stein estimator risk upper bound is: </p>
<span>$$\sigma^2 = \frac{1}{n}, ||\theta||^2 = ||\theta_{B_k}||^2, m = |B_k|$$</span><!-- Has MathJax -->
<p>Recall that we introduced the threshold index $I$ by considering bias-variance trade-off.<br>This time, we will try to find $K_0$ (the number of blocks) such that $I \le |B<em>1| + \cdots |B</em>{K_0}| \le 2I$ holds. </p>
<p>Now observe that for any positive real number $a$ and $b$, we have $\frac{ab}{a+b} \le a$ and $\frac{ab}{a+b} \le b$.<br>Therefore,</p>
<span>$$\sum_{k=1}^K \left[ \frac{|B_k| \frac{1}{n} ||\theta_{B_k}||^2}{|B_k|\frac{1}{n}+||\theta_{B_k}||^2} + 2 \frac{1}{n} \right] + O(n^{-\frac{2\alpha}{2\alpha+1}}) \\
\le \sum_{k=1}^{K_0} |B_k|\frac{1}{n} + \sum_{k=K_0+1}^K ||\theta_{B_k}||^2 + 2 \frac{1}{n} K + O(n^{-\frac{2 \alpha}{2\alpha+1}}) \\
\le \frac{1}{n} 2I + \sum_{i &gt; I}^{\infty} \theta_i^2 + + O(n^{-\frac{2 \alpha}{2\alpha+1}}) \\$$</span><!-- Has MathJax -->
<p>From the calculation in the last post, we see that this will be upper bounded by</p>
<span>$$\le c M^{\frac{1}{1+2\alpha}} n^{-\frac{2\alpha}{2\alpha+1}}$$</span><!-- Has MathJax -->
<p>One criticism toward this procedure is that the constant $c$ could be huge.<br>Is there a way to reduce it? We will explore alternative blockwise procedure next time.</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-13T00:56:57.000Z"><a href="/blog/2017/03/12/nonparametric-estimation/">2017-03-12</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2017/03/12/nonparametric-estimation/">Nonparametric Regression 01</a></h1>
  

    </header>
    <div class="entry">
      
        <p>The goal of this post is to show an instance of derivation of risk bound for nonparmetric regression.   </p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>For simplicity, we will focus on Gaussian sequence models. That is, our model of interest is the following type:</p>
<span>$$Y_i = \theta_i + \frac{1}{\sqrt{n}} \sigma Z_i, \text{ where } Z_i \sim^{iid} N(0,1)$$</span><!-- Has MathJax -->
<p>for $i = 1, …, n$, where we assume $\sigma=1$. In another word, we observe a noisy parameter $Y$, and we are interested in denoising $Y$ to get the true parameter $\theta$. </p>
<h2 id="Parameter-space"><a href="#Parameter-space" class="headerlink" title="Parameter space"></a>Parameter space</h2><p>Our parameter space is a type of Sobolev ellipsoid:<br><span>$$\Theta = \{ \theta : \sum_{i=1}^{\infty} i^{2 \alpha} \theta^2_i \le M \}$$</span><!-- Has MathJax --> </p>
<p>This way of choose our parameter space implicitly imposes smoothness assumption on function space we might be interested in. For example, if we view our estimation problem as a function estimation, the problem becomes to estimate a function $f(x)$ through a set of discrete sample pairs $(X_i, Y_i)$.<br>By Fourier basis expansion, </p>
<span>$$f(x) = \sum_{i=1}^{\infty} &lt;f, \varphi_i&gt; \varphi_i(x) \\
= \sum_{i=1}^{\infty} \theta_i \varphi_i(x)$$</span><!-- Has MathJax -->
<p>where <span>$\theta_i =  &lt;f, \varphi_i&gt;$</span><!-- Has MathJax --> and</p>
<span>$$\varphi_i(x) = 
\begin{cases} 
    cos(cix) \\
    sin(cix)
\end{cases}$$</span><!-- Has MathJax -->
<p>Imposing smoothness assumption on $f$ by bounding the integral of $f$’s second derivative is equivalent to the condition in our parameter space. That is, </p>
<span>$$f&apos;&apos;(x) = \sum_{i=1}^{\infty} \theta_i (\varphi_i(x))&apos;&apos; \\
= \sum_{i=1}^{\infty} \theta_i (ci)^2 \varphi_i(x)$$</span><!-- Has MathJax -->  
<p>So</p>
<span>$$\int (f&apos;&apos;(x))^2 dx \le M&apos; \iff \sum_{i=1}^{\infty} [\theta_i (ci)^2]^2 \le M&apos; \\
\sum_{i=1}^{\infty} c^4 i^4 \theta_i^2 \le M&apos;$$</span><!-- Has MathJax -->
<p>(In this derivation, we’ve assumed $\alpha=2$.)</p>
<p>Note that without any parameter space condition, the best estimation procedure is the linear procedure. We can show this through Pinsker bound, which we might go over in the future posts under decision-theory tag. </p>
<h2 id="Goal-of-inference-problem"><a href="#Goal-of-inference-problem" class="headerlink" title="Goal of inference problem"></a>Goal of inference problem</h2><p>Estimate $\theta$. Our loss function is </p>
<span>$$|| \hat{\theta} - \theta ||^2 = \sum_{i=1}^{\infty} (\hat{\theta}_i - \theta_i)^2$$</span><!-- Has MathJax -->
<h2 id="Estimation-Procedure"><a href="#Estimation-Procedure" class="headerlink" title="Estimation Procedure"></a>Estimation Procedure</h2><p>Intuitively, due to our parameter space restriction, when $i$ is large, $\theta_i$ has to become smaller, and at some point or later, it becomes neglible. This observation leads to the following naive estimator:</p>
<span>$$\hat{\theta}_i =  
\begin{cases} 
        Y_i &amp; i \le I  \\
        0    &amp; i &gt; I 
\end{cases}$$</span><!-- Has MathJax --> 
<p>where $I$ is the threshold value we will optimize (in terms of having a tigher risk lower bound) soon. </p>
<h2 id="Risk-Calculation"><a href="#Risk-Calculation" class="headerlink" title="Risk Calculation"></a>Risk Calculation</h2><p>With this procedure, we can easily calculate risk for each case. That is, when $\hat{\theta}_i = Y_i$, its risk is $E (Y_i - \theta_i)^2 = \frac{1}{n}$ (Note: this is just the variance of $Y_i$, which is $\frac{1}{n}$ from the definition of our model.) When $\hat{\theta}_i = 0$, its risk is $E (0 - \theta_i)^2 = \theta_i^2$ (Note that expectation is taken over data, and $\theta$ is deterministic value.) </p>
<p>With this simple algebra above, we observe that if $\theta^2_i \le \frac{1}{n}$, we want to use $\hat{\theta}_i = 0$, which is imposed by $i &gt; I$ condition. </p>
<p>The risk of this procedure is given by</p>
<span>$$R(\hat{\theta}) = E \sum_{i=1}^{\infty} (\hat{\theta}_i - \theta)^2 = E \sum_{i=1}^I (Y_i - \theta_i)^2 + E \sum_{i=I+1}^{\infty} \theta^2_i$$</span><!-- Has MathJax --> 
<h2 id="Risk-upper-bound"><a href="#Risk-upper-bound" class="headerlink" title="Risk upper bound"></a>Risk upper bound</h2><p>We immediately see that the first term is equivalent to $\frac{I}{n}$.<br>The question is how to upper bound the second term.<br>Recall the condition of our parameter space definition: $\sum_{i=0}^{\infty} i^{2 \alpha} \theta_i^2  \le M$. From this, we observe the following inequalities:</p>
<span>$$\sum_{i=0}^{\infty} i^{2 \alpha} \theta_i^2 \le M \\ 
\Rightarrow  \sum_{i = I + 1}^{\infty} i^{2 \alpha} \theta_i^2 \le M  \\ 
\Rightarrow (I+1)^{2\alpha} \sum_{i = I + 1}^{\infty} \theta_i^2 \le M \\ 
\Rightarrow \sum_{i = I + 1}^{\infty} \theta_i^2 \le \frac{M}{(I+1)^{2\alpha}} \le \frac{M}{I^{2 \alpha}}$$</span><!-- Has MathJax --> 
<p>So our upper bound is</p>
<span>$$R(\hat{\theta}) \le  \frac{I}{n} + \frac{M}{I^{2 \alpha}}$$</span><!-- Has MathJax --> 
<p>To find an optimal $I$, we will treat $\frac{M}{I^{2 \alpha}}$ as $Bias^2$, and pick $I$ to achieve the optimal variance-bias tradeoff. This leads to </p>
<span>$$\frac{I}{n} = \frac{M}{I^{2 \alpha}} \iff I^{1+2\alpha} = Mn \iff I = (Mn)^{\frac{1}{1+2\alpha}}$$</span><!-- Has MathJax --> 
<p>Therefore, </p>
<span>$$R(\hat{\theta}) \le \frac{I}{n} + \frac{M}{I^{2 \alpha}} = \frac{2}{n} (Mn)^{\frac{1}{1+2\alpha}} = 2 M^{\frac{1}{1+2\alpha}} n^{-\frac{2\alpha}{1+2\alpha}}$$</span><!-- Has MathJax --> 
<p>This leads to our risk upper bound:</p>
<span>$$sup_{\Theta} E || \hat{\theta} - \theta ||^2 \le  2 M^{\frac{1}{1+2\alpha}} n^{-\frac{2\alpha}{1+2\alpha}}$$</span><!-- Has MathJax --> 
<p>(We will try to find a better rate in some later posts.)</p>
<h2 id="Risk-lower-bound"><a href="#Risk-lower-bound" class="headerlink" title="Risk lower bound"></a>Risk lower bound</h2><p>To find lower bound, we resort to Le Cum’s idea. </p>
<p>We first construct a sub-parameter space so that the resulting risk calculation over the sub-parameter space is simple enough. Considering a sub-parameter space is useful because for any sub-parameter space $\Theta_0 \subset \Theta$, we have the following lower bound for sup risk for $\Theta$. </p>
<span>$$sup_{\Theta} E || \hat{\theta} - \theta||^2 \ge sup_{\Theta_0} E || \hat{\theta} - \theta ||^2$$</span><!-- Has MathJax --> 
<p>In our case, let our sub-parameter space be</p>
<span>$$\Theta_0 = \{ \theta : \theta_i = 0 \text{ if } i \ge I + 1, \theta_i = \frac{1}{\sqrt{n}} \text{ if } i \le I \}$$</span><!-- Has MathJax -->
<p>where $I$ is the previous optimal value : $I = (Mn)^{\frac{1}{1+2\alpha}}$. </p>
<p>We can confirm that <span>$\Theta_0$</span><!-- Has MathJax --> is indeed in <span>$\Theta$</span><!-- Has MathJax -->  by checking the condition <span>$\sum_{i=1}^{\infty} i^{2\alpha} \theta_i^2 \le M$</span><!-- Has MathJax -->: </p>
<span>$$\sum_{i=1}^I i^{2 \alpha} \frac{1}{n} = \frac{1}{n} \sum_{i=1}^I i^{2\alpha} \le \frac{1}{n} I I^{2\alpha} = \frac{1}{n} Mn = M$$</span><!-- Has MathJax --> 
<p>Now, we want to lower bound <span>$sup_{\Theta_0} E || \hat{\theta} - \theta ||^2$</span><!-- Has MathJax -->: </p>
<span>$$sup_{\Theta_0} E || \hat{\theta} - \theta ||^2 \ge  sup_{\Theta_0} E \sum_{i=1}^I \hat{\theta} - \theta ||^2  \\$$</span><!-- Has MathJax -->
<p>Now assume a prior on $\theta_i$ such that</p>
<span>$$\theta_i =
\begin{cases}
    0 &amp; \text{ w.p. } \frac{1}{2} \\
    \frac{1}{\sqrt{n}} &amp;  \text{ w.p. } \frac{1}{2} 
\end{cases}$$</span><!-- Has MathJax --> 
<p>Then, by observing that $sup_x f(x) \ge E_x f(x)$, we have </p>
<span>$$sup_{\Theta_0} E \sum_{i=1}^I || \hat{\theta} - \theta ||^2  \ge E_{\theta} E_{Y|\theta} [ \sum_{i=1}^I (\hat{\theta}_i - \theta_i )^2 ]$$</span><!-- Has MathJax -->
<p>Now recall that Bayes estimator is supposed to attain the smallest risk. This gives us the following further lower bound:</p>
<span>$$E_{\theta} E_{Y|\theta} [ \sum_{i=1}^I (\hat{\theta}_i - \theta_i )^2 ] \ge E_{\theta} E_{Y|\theta} [ \sum_{i=1}^I (\hat{\theta}_{i,Bayes} - \theta_i )^2 ]$$</span><!-- Has MathJax --> 
<p>Note that <span>$\hat{\theta}_{i,Bayes}$</span><!-- Has MathJax --> is a function of $Y_i$ only due to $i.i.d$ assumption. Therefore using our definition of prior,</p>
<span>$$E_{\theta} E_{Y|\theta} [ \sum_{i=1}^I (\hat{\theta}_{i,Bayes} - \theta_i )^2 ] 
= \sum_{i=1}^I [ \frac{1}{2} E_{Y_i|\theta_i=0} (\hat{\theta}_{i,Bayes} - 0)^2 + \frac{1}{2} E_{Y_i|\theta_i=\frac{1}{\sqrt{n}}} (\hat{\theta}_{i,Bayes} - \frac{1}{\sqrt{n}})^2 ]$$</span><!-- Has MathJax --> 
<p>Rewriting the above equation with $\phi_{a,b}(x)$ be the pdf of N(a,b) yields</p>
<span>$$\sum_{i=1}^I [ \frac{1}{2} E_{Y_i|\theta_i=0} (\hat{\theta}_{i,Bayes} - 0)^2 + \frac{1}{2} E_{Y_i|    \theta_i=\frac{1}{\sqrt{n}}} (\hat{\theta}_{i,Bayes} - \frac{1}{\sqrt{n}})^2  \\
= \sum_{i=1}^I \frac{1}{2} [ \int (\hat{\theta}_{i,Bayes} - 0)^2 \phi_{0,\frac{1}{n}}(x)dx + \int (\hat{\theta}_{i,Bayes} - \frac{1}{\sqrt{n}})^2  \phi_{\frac{1}{\sqrt{n}},\frac{1}{n}}(x) dx ]$$</span><!-- Has MathJax --> 
<p>Let <span>$g(x) = \min \{ \phi_{0,\frac{1}{n}}(x), \phi_{\frac{1}{\sqrt{n}},\frac{1}{n}}(x) \}$</span><!-- Has MathJax -->. Then using $g(x)$, the above equation can be lower bounded:</p>
<span>$$\ge \sum_{i=1}^I \frac{1}{2} \int \left( ( \hat{\theta}_{i,Bayes} - 0)^2 + (\hat{\theta}_{i,Bayes} - \frac{1}{\sqrt{n}})^2 \right)  g(x) dx$$</span><!-- Has MathJax -->
<p>Observing that <span>$a^2 + b^2 \ge \frac{1}{2} (a-b)^2$</span><!-- Has MathJax -->, we can rewrite it to</p>
<span>$$\ge \sum_{i=1}^I \frac{1}{2} \int \frac{1}{2} (\frac{1}{\sqrt{n}})^2 g(x) dx = \frac{I}{n} \frac{1}{4} \int g(x) dx$$</span><!-- Has MathJax -->
<p>Since $\int g(x) dx$ is some constant $c$, our risk lower bound is</p>
<span>$$c \frac{I}{n} = c n^{- \frac{2 \alpha}{2 \alpha + 1}}$$</span><!-- Has MathJax --> 
<p>This establishes our risk lower bound for this particular estimation procedure under this model. </p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-10T06:08:10.000Z"><a href="/blog/2017/03/10/notes-on-priors/">2017-03-10</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2017/03/10/notes-on-priors/">Notes on priors</a></h1>
  

    </header>
    <div class="entry">
      
        <p>A requirement </p>
<h2 id="The-Jeffreys-prior"><a href="#The-Jeffreys-prior" class="headerlink" title="The Jeffreys prior"></a>The Jeffreys prior</h2><ul>
<li>invariant under transformation. </li>
</ul>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-02-03T20:37:20.000Z"><a href="/blog/2017/02/03/mcmc/">2017-02-03</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2017/02/03/mcmc/">Notes on MCMC</a></h1>
  

    </header>
    <div class="entry">
      
        <p>MCMC (Markov Chain Monte Carlo) is a way to numerically sample from posterior distribution of interest by constructing Markov Chain in a smart way so that the stationary distribution of MC matches the desired posterior distribution.</p>
<p>The way I see it is to consider Markov Chain as a search on the parameter space. The goal is to find a parameter $\theta$ that specifies the desired posterior distribution $p(\theta|x)$. </p>
<p>To have a concrete picture, it’s useful to consider the parameter space (or should I say distribution space, which emphasizes that the space is invariance to reparametrization?) as a discrete space. </p>
<p>Let’s say we partition the parameter space into a countable blocks, and we start from a certain block at time 0.<br>Suppose at time $t$, we are at block $x$. Denote this probability as $P(\theta_t \in x)$ </p>
<p>How should we move? </p>
<ul>
<li><p>we should move toward regions of the parameter space with higher probability (with respect to the target distribution? But we don’t know this distribution do we..? Oh but we do know the likelihood and prior. So, although we cannot calculate the posterior distribution exactly, we are able to evaluate if the current point is higher than the next point, because this evaluation can be done by $\frac{p(\theta<em>{t+1})p(x|\theta</em>{t+1})}{p(\theta<em>{t})p(x|\theta</em>{t})}$; the integral constant will be cancelled out when you take the proportion.)</p>
</li>
<li><p>we should avoid the regions with lower probability with respect to the target distribution. </p>
</li>
</ul>
<p>Transition matrix T(x|y) $\iff$ Proposal distribution with pdf $g(x|x’)$ defined for all $x,x’ \in \xx$. </p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><p><a href="http://astrostatistics.psu.edu/su14/lectures/CosPop14-2-2-BayesComp-2.pdf" target="_blank" rel="external">http://astrostatistics.psu.edu/su14/lectures/CosPop14-2-2-BayesComp-2.pdf</a></p>
</li>
<li><p><a href="http://www2.stat.duke.edu/~km68/materials/214.7%20(MH).pdf" target="_blank" rel="external">http://www2.stat.duke.edu/~km68/materials/214.7%20(MH).pdf</a></p>
</li>
</ul>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-01-16T13:15:17.000Z"><a href="/blog/2017/01/16/krylov/">2017-01-16</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2017/01/16/krylov/">Krylov Subspace</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h3 id="Problem-setting"><a href="#Problem-setting" class="headerlink" title="Problem setting"></a>Problem setting</h3><ul>
<li><p>linear solver : $ Ax = b $</p>
</li>
<li><p>eigenvalue problem: $ Ax = \lambda x$</p>
</li>
</ul>
<p>Krylov subspace method is considered to be one of the hallmarks of modern numerical linear algebra. It is often effective when a matrix-vector product can be easily obtained (i.e. A is sparse, Ax can be accessed via one function call, etc.) There is also a nice convergence analysis. </p>
<h3 id="Krylov-subspace"><a href="#Krylov-subspace" class="headerlink" title="Krylov subspace"></a>Krylov subspace</h3><span>$K_d := \text{span } \{ b, Ab, A^2b, ..., A^{d-1}b \}$</span><!-- Has MathJax -->
<p>The high-level overview of the method consists of two components: 1. It projects the problem onto the Krylov subspace and solves the problem there. 2. Then projects the solution back to the original space. </p>
<p>Some characteristics: </p>
<ul>
<li>When $d$ increases, numerical errors will decrease (like all other iterative methods)</li>
<li>For large enough $d$, we can achieve the exact solution (as if direct methods would do!)</li>
<li>To avoid numerical errors, we need preconditioning in practice. </li>
</ul>
<h1 id="Arnoldi-process"><a href="#Arnoldi-process" class="headerlink" title="Arnoldi process"></a>Arnoldi process</h1><p>Goal: obtain a set of orthonormal vectors <span>$\{ q_1, ..., q_d \}$</span><!-- Has MathJax --> so that  <span>$span \{ q_1, ..., q_d \} = span \{ b, Ab, ..., A^db \}$</span><!-- Has MathJax --> via Gram-Schmidt type iterations using the recurrence : $AQ_d = Q_d H_d + r_d e_d^t$ (when $||r_d||$ gets small enough, we terminate the process), where $Q_d$ is a set of orthonormal vectors and $H_d$ is in a form of Hessenberg. For $A = A^T$, $H_d$ becomes tridiagonal (Lanczos process.)</p>
<h3 id="Projection-onto-Krylov-subspace"><a href="#Projection-onto-Krylov-subspace" class="headerlink" title="Projection onto Krylov subspace"></a>Projection onto Krylov subspace</h3><p>$H_d = Q_d^* A Q_d$</p>
<p>This matrix can be considered as the orthogonal projection of $A$ onto $K_d$ in the basis of ${ q_1, …, q_d }$. (Recall: similarity transformation = change of basis) </p>
<p>Because $H_d$ is in a Hessenberg form, which is computationally easier to work with, we might as well solve the problem of interest with respect to $H_d$ instead of $A$. </p>
<h3 id="Example-generalized-lesast-squares"><a href="#Example-generalized-lesast-squares" class="headerlink" title="Example: (generalized) lesast squares"></a>Example: (generalized) lesast squares</h3><p>Instead of working directly with $A$, we will project $A$ onto $K_d$ and obtain $H_d$. Then, the problem becomes : $ \hat{x} := \min || H_d x - \hat{b} ||$. We can obatin the solution in the original space by: $x = V^* \hat{x}$.   </p>
<h1 id="Application-to-training-Deep-Neural-Network"><a href="#Application-to-training-Deep-Neural-Network" class="headerlink" title="Application to training Deep Neural Network?"></a>Application to training Deep Neural Network?</h1><p>In the next post, I’ll go over how Krylov subspace method can be applied to deep learning. </p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-01-13T01:21:16.000Z"><a href="/blog/2017/01/12/gan-bss/">2017-01-12</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2017/01/12/gan-bss/">Blind-Source Separation using Generative Adversarial Network</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The goal of this experiment is to see if blind-source separation can be solvable in an unsupervised fashion with an aid from pre-trained GAN. </p>
<p>The application of this model, if correctly implemented, can be extended to image separation and audio processing. For image processing: Given a mixture of two images, can we recover the original two images? For audio processing: Given the input of duet of piano and guitar, can we construct a model that separates piano from guitar in an unsupervised manner? </p>
<h1 id="Generative-Adversarial-Network"><a href="#Generative-Adversarial-Network" class="headerlink" title="Generative Adversarial Network"></a>Generative Adversarial Network</h1><p>GAN is a generative model that can be used to perform a very sophisticated high dimensional density estimation using a learning framework which mimics a Two-Player adversarial game.   </p>
<p>Density estimation can be done by $G$, which represents a generative model, usually constructed by neural networks. Specifically, we consider $G: z \rightarrow x$ where $z$ is drawn from some known distribution (usually uniform distribution) and $x$ is a point in a high dimensional space. $x$ can be viewed as a realization of a random variable $X$ that is drawn from a learned probability distribution (modeled by $G$) over the high dimensional space. As an example, suppose we want to model the underlying probability distribution of images. We can consider the domain of $X$ to be the entire image space, and choose $G$ to be a sophisticated CNN that can upsample a simple uniformly random vector $z$ (usually lives in some low dimension) to an image $x$ (which usually lives in a very high dimensional space). By using the adversarial-game type learning framework, we can learn a model that represents the probability distribution over the entire images.  </p>
<p>Now the question is how exactly we construct such a function that can model complex probability distribution. The phrase “two player adversarial game” sort of tells us that we should consider another model other than $G$, and somehow creates the adversarial game between those two models. Specifically, we introduce another model $D$, which represents a discriminative model s.t. $D: x \rightarrow [0,1]$, where $x$ is a point in the same high-dimensional space as the range of $G$. The adversarial game can be realized by assigning to the output of $D$ a probability of $x$ coming from a true distribution. So if the output of $D$ is low, it means that $D$ thinks that $x$ comes from a “fake” distribution modeled by $G$. $G$ tries to “fool” $D$ by generating a sample that looks like one from the true distribution, whereas $D$ attempts to detect “fake” samples generated by $G$. We iteratively update $G$ and $D$ until we are satisfied with the quality of the results from $G$.   </p>
<p>How can we design an objective function that realizes the above adversarial-game type learning framework? The simplest way is to use min-max type algorithm. Recall that we want $D(x)$ to produce high values when <span>$x \sim p_{true}$</span><!-- Has MathJax --> and low values when <span>$x \sim p_{fake}$</span><!-- Has MathJax -->. This can be realized by <span>$\max_{D}  E_{x \sim p_{data}} [ \log D(x) ] + E_{z \sim p_{predefined}} [ \log( 1 - D(G(z))) ]$</span><!-- Has MathJax -->. We also want $G(z)$ to make $D(G(z))$ as high as possible, which can be realized by applying $min_{G}$ operation to the above terms. This amounts to the following objective funtion:</p>
<span>$J(\theta_G, \theta_D) =  min_{G} max_{D}  E_{x \sim p_{data}} [ log D(x) ] + E_{z \sim p_{predefined}} [ log 1 - D(G(z)) ]$</span><!-- Has MathJax --> 
<h1 id="Blind-Source-separation"><a href="#Blind-Source-separation" class="headerlink" title="Blind-Source separation"></a>Blind-Source separation</h1><p>The central question of BSS is this: Given an observation that is a mix of a number of different sources, can we recover both the underlying mechanism of such mixing and the sources, having access to the observation only?</p>
<p>In general, the answer is “no”, because the problem is too difficult to solve. But if we add some conditions on the properties of sources, then we could recover both when the mixing mechanism can be considered as a linear system.  </p>
<p>Here, we consider a non-linear version of the BSS problem. That is, given an observation $x$, we assume $x$ is generated from an unknown non-linear function $F$ with an unknown input $z$. </p>
<p>To make it suitable for our image experiment, suppose that we mix two different images, say, MNIST data and some synthesized data. We assume that these two images live in two different data manifolds. The sources $z$ here are precisely these two manifolds, and the non-linear function $F$ is the generating process of the mixture image $x$ from $z$s. The question is, when we observe only $x$, which is a mixture of two images, can we recover $F$, $z_1$ and $z_2$, (and thus recover two original images) by modeling a part of $F$ as GAN? (We don’t have access to the underlying manifold, so in our experiment the recovery of $z_1$ and $z_2$ will be done implicitly.)</p>
<h1 id="Preliminary-Model"><a href="#Preliminary-Model" class="headerlink" title="Preliminary Model"></a>Preliminary Model</h1><p>Essentially, we attempt to solve the problem by using autoencoder, where the decoder part consists of two pre-trained GANs + an additional decoder that converts the output of two GANs into one image that is an estimate of the original input, which is the mixture of the two images.    </p>
<ol>
<li>Pre-training:  Let G’(z) and G(z) are the two generative models learned by GAN’s framework using MNIST and synthesized data, respectively. </li>
<li>Build an autoencoder using G’(z) and G(z) as a part of decoder.  </li>
</ol>
<p>We can model $G’(z)$ and $G(z)$ by DCGAN. For implementation, I should be careful how to make a good alignment between the two-dimensional $z$ with the encoder. (Two-dimensional because we have two images.) </p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>(To be followed.)</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-01-13T00:15:57.000Z"><a href="/blog/2017/01/12/notes-nn-optim/">2017-01-12</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2017/01/12/notes-nn-optim/">Notes on neural network optimization</a></h1>
  

    </header>
    <div class="entry">
      
        <p><em>This is an on-going notes and subject to change. Last updated: 2017/01/13</em></p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>Optimization is inherently tied with machine learning because what “learning” means is essentially about minimizing an objective function associated with the problem of interest. In fact, the resurgence of neural network stems from various inventions that allow neural network optimization easier, faster, and better (generalization); Dropout, ReLU activation, Batch Normalization, LSTM (for RNN to avoid gradient explosion/vanishing) to name a few. </p>
<p>One of the fundamental questions with regard to optimization in deep neural network is the following: Why is finding local minima enough? Since it is virtually impossible to find analytical solutions in high dimensional non-convex optimization problems, we have to rely on iterative methods, which doesn’t necessarily gurantee to give us good solutions. The dominanting optimization algorithms in deep learning as of January 2017 are all variants of stochasitc gradient descent; Adam, AdaGrad, and RMSProp etc. However, since SGD uses only local information to update the current estimates, it is likely that the algorithm will get stuck around local minima. In practice, however, local minima found by SGD-type algorithms with appropriate hyperparameter tuning are good enough to achieve impressive results in many real tasks.  </p>
<p>It is hypothesized that the reason why local minima are good enough is that there is no “bad” local minima. That is, all local minima are very close to global minima in the error surface of deep neural network. For deep linear models, there is <a href="https://arxiv.org/abs/1605.07110" target="_blank" rel="external">a recent paper</a> that shows all local minima are global minima under some assumptions (which are not satisfied by models used in real tasks). </p>
<p>For deep neural network, it is partially backed up by Dauphin’s paper. Recent progress on statistical physics and random matrix theory show that the error surface of random Gaussian fields have interesting structure; most of local minima are close to global minima. Based on these results, Dauphin hypothesized that the error surface of neural network also follows a similar structure when the number of parameters is huge. This shows that the answer might be due to the scale of the neural network. </p>
<h3 id="Dauphin’s-arguments-with-regard-to-saddle-points"><a href="#Dauphin’s-arguments-with-regard-to-saddle-points" class="headerlink" title="Dauphin’s arguments with regard to saddle points"></a>Dauphin’s arguments with regard to saddle points</h3><p>Dauphin’s argument behind his proposed algorithm goes like this: “The reason why many algorithms seem to get stuck during training is because of saddle points surrounded by high error flat regions, which looks as if the algorithm is stuck at high error local minima. So we developed an algorithm that escapes from these high error flat regions.”  </p>
<p>I wasn’t sure how realible the first part of their statement; optimizing deep neural network gets stuck at high error surface. It might be from the views based on past research (~2011) where we didn’t have effective tools like BN and ReLU. (Indeed, their experiments only deal with deep auto-encoder.)   </p>
<p>I think as of now, we have an updated view on neural network optimization. In fact, Goodfellow &amp; Vinyals (2015) show that typical deep network are easy to optimize and can achieve near-zero loss on the training set. (What’s hard is to find the ones with good generalization error, which will be discussed in Section 3.)</p>
<p>Although Dauphin’s claim for saddle points might not exactly work for deep neural network, this saddle points hypothesis was certainly a driving force for many interesting non-convex optim papers that recently appeared outside of deep learning community. One of the impressive results is <a href="https://arxiv.org/pdf/1503.02101v1.pdf" target="_blank" rel="external">this paper</a> by Rong Ge, which provides the answer to the local v.s. global minima question in the context of Matrix Completion. The same author also shows that SGD converges to local minima in polynomial time for Tensor Factorization. Another approach is taken by John Wright, Micheal Jordan, etc, but I haven’t follow their papers too closely.  </p>
<h1 id="2-Degenerate-Hessian"><a href="#2-Degenerate-Hessian" class="headerlink" title="2. Degenerate Hessian"></a>2. Degenerate Hessian</h1><p>Most of the results to show “local minima are global minima”-type arugments assume that loss functions do not have degenerate Hessian. Degenerate Hessians are the ones that has more than one eigelvalue being 0. In deep neural network, we have degenerate Hessian everywhere, which is why we can’t simply apply the results from the above works to deep neural network.</p>
<p>(Side notes: this is also why deep learning is hard to analyze using classical statistics and decision theory framework, which heavily relies on the fact that Hessian being non-singular. This assures asymptotic normality for posterior distribution.)</p>
<p><a href="https://arxiv.org/pdf/1611.07476.pdf" target="_blank" rel="external">The recent paper</a> from Facebook discusses the consequence of degenerate Hessian specifically in deep learning, so I’ll summerize the main points in the follwing:</p>
<ul>
<li>Eigenvalue spectrum composes two parts: the bulk around 0 and the edges, where it is hypothesized that the former implies the over-parametrization of the model and the latter indicates the complexity of the input data. </li>
</ul>
<p><a href="https://arxiv.org/pdf/1611.01838v3.pdf" target="_blank" rel="external">The recent paper</a> by Chaudhari gives more details on the characteristics of the scale of the values at the edge of eigenvalue spectrum; they show that positive eigenvalues have a long tail, and negative eigenvalues have much faster decay. </p>
<p>According to Chaudhari, this trend is ubiquitous across a variety of network architectures, sizes, datasets, or optimization algorithms, which suggests that “local minima that generalize well and are discovered by gradient descent lie in “wide valleys” of energy landscape”, which are characterized by degenerate Hessians.</p>
<h1 id="3-Generalization-performance"><a href="#3-Generalization-performance" class="headerlink" title="3. Generalization performance"></a>3. Generalization performance</h1><p>One important thing we need to remember is that our goal is not to find the global minima of the objective function of interest, but to find good enough minima that has high generalization performance. We don’t want to overfit our model to the training data. </p>
<p>Motivated by the observation in the above section, Chaudhari proposed <a href="https://arxiv.org/pdf/1611.01838v3.pdf" target="_blank" rel="external">Entropy-SGD</a>, which is actively seeking flat regions (with low error), as opposed to Dauphin’s algorithm, which intentionally escapes from saddle points. </p>
<p>[details of Entropy-SGD will be followed.]</p>
<h3 id="Batch-Normalization-and-Generalization-Performance"><a href="#Batch-Normalization-and-Generalization-Performance" class="headerlink" title="Batch Normalization and Generalization Performance"></a>Batch Normalization and Generalization Performance</h3><p><a href="https://arxiv.org/abs/1511.06747" target="_blank" rel="external">This paper</a> provides a unified framework that generalizes Batch Normalization and other methods (path-normalized approach).<br>Is there theoretical argument as to the generalization power of Batch Normalization? I often heard the phrase like: “If you use BN, you don’t need to use Dropout.” This seems to be problem-dependent at least for me. Can we say something like, Batch Normalization allows the network to converge to flat regions with low error (and thus high generalization performance)?</p>
<h1 id="4-1st-order-v-s-2nd-order"><a href="#4-1st-order-v-s-2nd-order" class="headerlink" title="4. 1st order v.s. 2nd order"></a>4. 1st order v.s. 2nd order</h1><p>If we could implement Natural Gradient Descent in a large-scale setting, then it’s ideal for good generalization performance and invariance properties.<br>(For details, see <a href="https://papers.nips.cc/paper/3234-topmoumoute-online-natural-gradient-algorithm.pdf" target="_blank" rel="external">this paper</a></p>
<p>The issue is that we can’t really work with Natural Gradient because computing full Fisher information matrix is prohibitive, and at present it seems that the optimization algorithms based on approximation of Fisher information matrix are not working well enough to be competitive with Adam or tuned SGD, etc, which is why 2nd-order methods are active research area. Essentially, the matrix multiplied by the gradient vector in the GD update equation in the 2nd-order method can be considered as an approximation to Fisher information matrix, so the goal of 2nd-order methods research is (in a way) to come up with a computationally feasible method to approximate Fisher information matrix.  </p>
<p>Martens and Pascanu have many interesting works on 2nd-order methods in deep neural network optimization. If you are interested in 2nd order methods, I highly recommend reading James Martens PhD thesis; it is a very good introduction and overview of the field. </p>
<p>Below, I’ll list some of the important papers:</p>
<ul>
<li><a href="http://www1.icsi.berkeley.edu/~vinyals/Files/vinyals_aistats12.pdf" target="_blank" rel="external">Krylov Subspace Descent in Deep Learning by Vinyals</a> (2011)</li>
<li><a href="http://icml2010.haifa.il.ibm.com/papers/458.pdf" target="_blank" rel="external">Hessian-Free optimization by Martens</a> (2011)</li>
<li><a href="https://arxiv.org/pdf/1206.6464.pdf" target="_blank" rel="external">Estimating the Hessian by Back-propagating Curvature by Martens</a>  </li>
<li><a href="https://arxiv.org/pdf/1503.05671.pdf" target="_blank" rel="external">K-FAC algorithm by Martens and Grosse</a>: New 2nd order method that attempts to mimic Natural Gradient by constructing an invertible approximation of Fisher information matrix in an online fashion. (2016) </li>
</ul>
<p>I’ll cite some of the comments from the following Reddit thread with regards to why L-BFGS is not used in deep learning area very often:<br><a href="https://www.reddit.com/r/MachineLearning/comments/4bys6n/lbfgs_and_neural_nets/" target="_blank" rel="external">https://www.reddit.com/r/MachineLearning/comments/4bys6n/lbfgs_and_neural_nets/</a></p>
<hr>
<p>“Back in 2011 when that paper was published, deep learning honestly didn’t work all that well on many real tasks.<br>One of the hypotheses at the time (which has since been shown to be false) is the optimization<br>problem that neural nets posed was simply too hard – neural nets are non-convex, and<br>we didn’t have much good theory at the time to show that learning with them was possible.<br>That’s one of the reasons why people started exploring different optimization algorithms for neural nets, which was a trend that continued roughly until the breakthrough results in 2012, which worked remarkably well despite only using SGD + momentum. Since then, more theory has been developed supporting this, and other tricks have been developed (BatchNorm, RMSProp/Adagrad/Adam/Adadelta) that make learning easier.”</p>
<hr>
<h1 id="Future-Directions-Optimization-for-Two-Player-game"><a href="#Future-Directions-Optimization-for-Two-Player-game" class="headerlink" title="Future Directions: Optimization for Two-Player game"></a>Future Directions: Optimization for Two-Player game</h1><p>I think Plateau-finding type alogirhtms will keep appearing in 2017. How to characterize plateau efficiently will be a key. As for the 2nd-order method, how to construct non-diagonal scaling (that will be applied to gradient vector) with low per-iteration cost is a main challenge. (Notes to myself: can we use an idea from the hessian-sketch paper to construct something like Fisher-sketch, an approximation to (invertible) Fisher information matrix? ) </p>
<p>Until now, we only talk about optimizing one objective function.<br>Generative Adversarial Network is a relatively new generative model that uses a two-player learning framework for high dimensional density estimation.<br>Although it already produces impressive results in generating images, there are several issues. One such problem is that training GAN is notroiusly hard due to the two-player nature; optimizing one function is not necessary an optimal update for the other function. Effective training scheme for GAN is certainly open for future research. </p>
<p>[Things to add: # Small minibatches are better for generalization]<br>[I should write a post for each algorithm/paper that appears in this post and just add a link for each so that this post can be more concise / easier to mantain?] </p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-10-17T13:46:43.000Z"><a href="/blog/2016/10/17/learningtolearn-1/">2016-10-17</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/10/17/learningtolearn-1/">Learning-To-Learn: RNN-based optimization</a></h1>
  

    </header>
    <div class="entry">
      
        <p>Around the middle of June, this paper came up: <a href="https://arxiv.org/pdf/1606.04474v1.pdf" target="_blank" rel="external">Learning to learn by gradient descent by gradient descent</a>. For someone who’s interested in optimization and neural network, I think this paper is particularly interesting. The main idea is to use neural network to tune the learning rate for gradient descent.</p>
<h2 id="Summary-of-the-paper"><a href="#Summary-of-the-paper" class="headerlink" title="Summary of the paper"></a>Summary of the paper</h2><p>Usually, when we want to design learning algorithms for an arbitrary problem, we first analyize the problem, and use the insight from the problem to design learning algorithms. This paper takes a one-level-above approach to algorithm design by considering a class of optimization problems, instead of focusing on one particular optimization problem. </p>
<p>The question is how to learn an optimization algorithm that works on a “class” of optimization problems. The answer is by parameterizing the optimizer. This way, we effectively cast algorithm design as a learning problem, in which we want to learn the parameters of our oprimizer (, which we call the optimzee parameters.) </p>
<p>But how do we model the optimizer? We use Recurrent Neural Network. Therefore, the parameters of the oprimizer are just the parameters of RNN. The parameters of the original function in question (i.e. the cost function of “one instance” of a problem that is drawn from a class of optimization problems) are referred as “optimizee parameters”, and are updated using the output of our optimizer, just as we update parameters using the gradient in SGD. The final optimzee parameters $\theta^*$ will be a function of the optimizer parameters and the function in question. In summary:</p>
<p>$$\theta^* (\phi, f) \text{: the final optimzee parameters}$$ </p>
<p>$$\phi \text{: the optimizer parameters}$$ </p>
<p>$$ f\text{: the function in question} $$</p>
<p>$$<br>\theta_{t+1} = \theta_t + g_t(\nabla f(\theta_t), \phi)  \text{: the update equation of the optimzee parameters}<br>$$<br>where $g_t$ is modeled by RNN. So $\phi$is the parameter of RNN. Because LSTM is better than vanilla RNN in general (citation needed*), the paper uses LSTM. Regular gradient descent algorithms use $g_t(\nabla f(\theta_t), \phi) = -\alpha \nabla f(\theta_t)$. </p>
<p>RNN is a function of the current hidden state $h_t$, the current gradient $\nabla f(\theta_t)$, and the current parameter $\phi$.</p>
<p>The “goodness” of our optimizer can be measured by the expected loss over the distribution of a function $f$, which is</p>
<p>$$ L(\phi) = \mathbb{E}_f [f(\theta^* (\phi, f))] $$</p>
<p>(I’m ignoring $w_t$ in the above expression of $L(\phi)$ because in the paper they set $w_t = 1$.)</p>
<p>For example, suppose we have a function like $f(\theta) = a \theta^2 + b\theta + c$. If $a,b,c$ are drawn from the Gaussian distribution with some fixed value of $\mu$ and $\sigma$, the distribution of the function $f$ can be defined. (Here, the class of optimization problem is a function where $a,b,c$ are drawn from Gaussian.) In this example, the optimzee parameter is $\theta$. The optimizer (i.e. RNN) will be trained by optimizing functions which are randomly drawn from the function distribution, and we want to find the best parameter $\theta$. If we want to know how good our optimizer is, we can just take the expected value of $f$ to evaluate the goodness, and use gradient descent to optimize this $L(\phi)$.</p>
<p>After understanding the above basics, all that is left is some implementation/architecture details for computational efficieny and learning capability. </p>
<p>(By the way, there is a typo in page 3 under Equation 3; <span>$\nabla_{\theta} h(\theta)$</span><!-- Has MathJax --> should be <span>$\nabla_{\theta} f(\theta)$</span><!-- Has MathJax -->. Otherwise it doesn’t make sense.)</p>
<h3 id="Coordinatewise-LSTM-optimizer"><a href="#Coordinatewise-LSTM-optimizer" class="headerlink" title="Coordinatewise LSTM optimizer"></a>Coordinatewise LSTM optimizer</h3><img src="/blog/2016/10/17/learningtolearn-1/compgraph.png" alt="title" title="title">
<p>[The Figure is from the paper : Figure 2 on page 4]</p>
<p>To make the learning problem computationally tractable, we update the optimzee parameters $\theta$ coordinatewise, much like other successful optimization methods such as Adam, RMSprop, and AdaGrad. </p>
<p>To this end, we create $n$ LSTM cells, where $n$ is the number of dimensions of the parameter of the objective function. We setup the architecture so that the parameters for LSTM cells are shared, but each has a different hidden state. This can be achieved by the code below: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">lstm = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(number_of_coordinates):</div><div class="line">    cell_list[i] = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_layers) <span class="comment"># num_layers = 2 according to the paper.</span></div></pre></td></tr></table></figure>
<h3 id="Information-sharing-between-coordinates"><a href="#Information-sharing-between-coordinates" class="headerlink" title="Information sharing between coordinates"></a>Information sharing between coordinates</h3><p>The coordinatewise architecture above treats each dimension independently, which ignore the effect of the correlations between coordinates. To address this issue, the paper introduces more sophisticated methods. The following two models allow different LSTM cells to communicate each other. </p>
<ol>
<li>Global averaging cells: a subset of cells are used to take the average and outputs that value for each cell.</li>
<li>NTM-BFGS optimizer: More sophisticated version of 1., with the external memory that is shared between coordinates.</li>
</ol>
<h2 id="Implementation-Notes"><a href="#Implementation-Notes" class="headerlink" title="Implementation Notes"></a>Implementation Notes</h2><h3 id="Quadratic-function-3-1-in-the-paper"><a href="#Quadratic-function-3-1-in-the-paper" class="headerlink" title="Quadratic function (3.1 in the paper)"></a>Quadratic function (3.1 in the paper)</h3><p>Let’s say the objective funtion is $f(\theta) = || W \theta - y ||^2$, where the elements of $W$ and $y$ are drawn from the Gaussian distribution.</p>
<p>$g$ (as in $\theta_{t+1} = \theta_t + g$) has to be the same size as the parameter size. So, it will be something like:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">g, state = lstm(input_t, hidden_state) <span class="comment"># here, input_t is the gradient of a hidden state at time t w.r.t. the hidden</span></div></pre></td></tr></table></figure>
<p>And the update equation will be:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">param = param + g</div></pre></td></tr></table></figure>
<p>The objective function is:<br><span>$$L(\phi) = \mathbb{E}_f [ \sum_{t=1}^T w_t f(\theta_t) ]$$</span><!-- Has MathJax --><br><span>$$\text{where,  }\theta_{t+1} = \theta_t + g_t$$</span><!-- Has MathJax --><br><span>$$\left[
    \begin{array}{c}
      g_t \\
      h_{t+1}  
    \end{array}
\right]
= RNN(\nabla_t, h_t, \phi)$$</span><!-- Has MathJax --></p>
<p>The loss $L(\phi)$ can be computed by double-for loop. For each loop, a different function is randomly sampled from a distribution of $f$. Then, $\theta_t$ will be computed by the above update equation. So, overall, what we need to implement is the two-layer coordinate-wise LSTM cell. The actual implementation is <a href="https://github.com/runopti/Learning-To-Learn" target="_blank" rel="external">here</a>.  </p>
<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><img src="/blog/2016/10/17/learningtolearn-1/output_38_1.png" alt="title" title="title">
<p>I compared the result with SGD, but SGD tends to work better than our optimizer for now. Need more improvements on the optimization…</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-09-04T18:21:09.000Z"><a href="/blog/2016/09/04/induced-norm/">2016-09-04</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/09/04/induced-norm/">Induced Matrix Norm</a></h1>
  

    </header>
    <div class="entry">
      
        <p>Notes on Induced Matrix Norm. I think the name comes from that fact that it is “induced” by a vector.   </p>
<p><strong>Definition:</strong><br>For a $m$ by $n$ matrix $A$, and a n-dimensional vector $x$,<br><span>$$|| A ||_p = \sup \{ ||Ax||_p : ||x||_p = 1\}$$</span><!-- Has MathJax --></p>
<p>Note that since given $A$, $ ||Ax||_p$ is a function of $x$, which is closed and bounded as $||x||_p = 1$, $||Ax||_p$ achieves maximum or minimum on some $x$. So we can replace $\sup$ with $\max$. </p>
<p><strong>Claim I : $p = 1$:</strong><br><span>$$||A||_1 = \max_{1 \le j \le n} \sum_{i=1}^m a_{ij}$$</span><!-- Has MathJax --></p>
<hr>
<p>First, let us find some upper bound on $||Ax||_1$.<br><span>$$||Ax||_1 =  || \sum_{i=1}^n A_i x_i ||_1 \le \sum_{i=1}^n |x_i| || A_i ||_1 
\le \max_j ||A_j|| \sum_{i=1}^n |x_i| = \max_j ||A_j|| ||x||_1$$</span><!-- Has MathJax --></p>
<p>So given A, we have found a constant $K = \max_j ||A_j||$ that will upper bound $||Ax||_1$ by  $K||x||_1$:<br><span>$$||Ax||_1 \le  K ||x||_1$$</span><!-- Has MathJax --></p>
<p>Next, we show that there exists $x$ for which we have equality for the above equation. Since $K = \max_j ||A_j||$, we can just let $x = [0, 0, …, 1, .. 0, 0]$ where 1 is at the $k$th position. $k$ is the index that satisfies $||A_k|| = \max_j ||A_j||$.    </p>
<p>So we’ve found that ||A||_1 = \max_j ||A_j|| . </p>
<p><strong>Claim II : $p = \infty$:</strong><br><span>$$||A||_{\infty} = \max_{1 \le i \le m} \sum_{j=1}^n a_{ij}$$</span><!-- Has MathJax --></p>
<hr>
<p>First, consider <span>$|| Ax ||_{\infty}$</span><!-- Has MathJax -->. Then,</p>
<span>$$\| Ax \|_{\infty} = 
\left \|
 \begin{matrix}
  \sum_{j=1}^n a_{1j} x_j  \\
  \sum_{j=1}^n a_{2j} x_j  \\
  \vdots \\
  \sum_{j=1}^n a_{mj} x_j
 \end{matrix}
\right \|_{\infty} = \max_i \left | \sum_{j=1}^n a_{ij} x_j \right | 
\le \max_i \sum_{j=1}^n \left | a_{ij} x_j \right | 
\le \max_i \sum_{j=1}^n \left | a_{ij} \right | \max_k \left | x_k \right |
= \max_i \left( \sum_{j=1}^n \left | a_{ij} \right | \right) \| x \|_{\infty}$$</span><!-- Has MathJax --> 
<p>So given $A$, we have found a constant <span>$K = \max_i \left( \sum_{j=1}^n \left | a_{ij} \right | \right)$</span><!-- Has MathJax --> that will upper bound <span>$||Ax||_{\infty}$</span><!-- Has MathJax -->:<br><span>$$\| Ax \|_{\infty} \le  K &nbsp;\| x \|_{\infty}$$</span><!-- Has MathJax --> </p>
<p>To find <span>$\| A \|_{\infty}$</span><!-- Has MathJax -->, we need to find some $x$ that equates the above inequality, which is fortunately straightforward as in the case of $p = 1$. By examining :</p>
<span>$$\max_i \left | \sum_k a_{ik} x_k \right | = K \max_k | x_k |$$</span><!-- Has MathJax --> 
<p>we notice that we achieve the equality if we let<br><span>$$x_k = \begin{cases}
  \frac{ a_{ik} }{ | a_{ik} |} &amp; (a_{ik} \neq 0) \\
  1  &amp;  (a_{ik} = 0)
\end{cases}$$</span><!-- Has MathJax --></p>
<p>where $k$ is the index such that <span>$A_{k,:} = \max_i \left( \sum_{j=1}^n \left | a_{ij} \right | \right)$</span><!-- Has MathJax -->  <strong><em>Note:</em></strong>  I’m using $A_k$ as denoting the $k$ th column vector of a matrix $A$, and <span>$A_{k, :}$</span><!-- Has MathJax --> as denoting the $k$ th row vector of $A$. </p>
<p><strong>Claim III : $p = 2$:</strong><br><span>$$\|A\|_2 =  \sqrt{ \lambda_{\max} }$$</span><!-- Has MathJax --><br>where $\lambda_{\max}$ is the largest eigenvalue of $A^T A$. </p>
<hr>
<p>This is the spectral norm! </p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://en.wikipedia.org/wiki/Matrix_norm" target="_blank" rel="external">https://en.wikipedia.org/wiki/Matrix_norm</a></li>
<li><a href="http://www.ece.uah.edu/courses/ee448/chapter4.pdf" target="_blank" rel="external">http://www.ece.uah.edu/courses/ee448/chapter4.pdf</a></li>
</ul>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
  
    <a href="/blog/page/2/" class="alignright next">Siguiente</a>
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Buscar">
    <input type="hidden" name="q" value="site:runopti.github.io/blog">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">Etiquetas</h3>
  <ul class="entry">
  
    <li><a href="/blog/tags/PRML/">PRML</a><small>1</small></li>
  
    <li><a href="/blog/tags/cv/">cv</a><small>1</small></li>
  
    <li><a href="/blog/tags/decision-theory/">decision-theory</a><small>3</small></li>
  
    <li><a href="/blog/tags/math/">math</a><small>7</small></li>
  
    <li><a href="/blog/tags/neuralnet/">neuralnet</a><small>8</small></li>
  
    <li><a href="/blog/tags/optimization/">optimization</a><small>5</small></li>
  
    <li><a href="/blog/tags/paper-memo/">paper-memo</a><small>2</small></li>
  
    <li><a href="/blog/tags/research/">research</a><small>0</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2017 Yutaro Yamada
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/blog/js/jquery.imagesloaded.min.js"></script>
<script src="/blog/js/gallery.js"></script>




<link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/blog/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]],"processEscapes":true}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
