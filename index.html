<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Notes</title>
  <meta name="author" content="Yutaro Yamada">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Notes"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/blog/favicon.png" rel="icon">
  <link rel="alternate" href="/blog/atom.xml" title="Notes" type="application/atom+xml">
  <link rel="stylesheet" href="/blog/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/blog/">Notes</a></h1>
  <h2><a href="/blog/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/blog/">Home</a></li>
    
      <li><a href="/blog/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-01-12T11:21:16.000Z"><a href="/blog/2017/01/12/gan-bss/">2017-01-12</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2017/01/12/gan-bss/">Blind-Source Separation using Generative Adversarial Network</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>The goal of this experiment is to see if blind-source separation can be solvable in an unsupervised fashion with an aid from pre-trained GAN. </p>
<p>The application of this model, if correctly implemented, can be extended to image separation and audio processing. For image processing: Given a mixture of two images, can we recover the original two images? For audio processing: Given the input of duet of piano and guitar, can we construct a model that separates piano from guitar in an unsupervised manner? </p>
<h1 id="Generative-Adversarial-Network"><a href="#Generative-Adversarial-Network" class="headerlink" title="Generative Adversarial Network"></a>Generative Adversarial Network</h1><p>GAN is a generative model that can be used to perform a very sophisticated high dimensional density estimation using a learning framework which mimics a Two-Player adversarial game.   </p>
<p>Density estimation can be done by $G$, which represents a generative model, usually constructed by neural networks. Specifically, we consider $G: z \rightarrow x$ where $z$ is drawn from some known distribution (usually uniform distribution) and $x$ is a point in a high dimensional space. $x$ can be viewed as a realization of a random variable $X$ that is drawn from a learned probability distribution (modeled by $G$) over the high dimensional space. As an example, suppose we want to model the underlying probability distribution of images. We can consider the domain of $X$ to be the entire image space, and choose $G$ to be a sophisticated CNN that can upsample a simple uniformly random vector $z$ (usually lives in some low dimension) to an image $x$ (which usually lives in a very high dimensional space). By using the adversarial-game type learning framework, we can learn a model that represents the probability distribution over the entire images.  </p>
<p>Now the question is how exactly we construct such a function that can model complex probability distribution. The phrase “two player adversarial game” sort of tells us that we should consider another model other than $G$, and somehow creates the adversarial game between those two models. Specifically, we introduce another model $D$, which represents a discriminative model s.t. $D: x \rightarrow [0,1]$, where $x$ is a point in the same high-dimensional space as the range of $G$. The adversarial game can be realized by assigning to the output of $D$ a probability of $x$ coming from a true distribution. So if the output of $D$ is low, it means that $D$ thinks that $x$ comes from a “fake” distribution modeled by $G$. $G$ tries to “fool” $D$ by generating a sample that looks like one from the true distribution, whereas $D$ attempts to detect “fake” samples generated by $G$. We iteratively update $G$ and $D$ until we are satisfied with the quality of the results from $G$.   </p>
<p>How can we design an objective function that realizes the above adversarial-game type learning framework? The simplest way is to use min-max type algorithm. Recall that we want $D(x)$ to produce high values when $x \sim p<em>{true}$ and low values when $x \sim p</em>{fake}$. This can be realized by $max<em>{D}  E</em>{x \sim p<em>{data}} [ log D(x) ] + E</em>{z \sim p<em>{predefined}} [ log 1 - D(G(z)) ]  $. We also want $G(z)$ to make $D(G(z))$ as high as possible, which can be realized by applying $min</em>{G}$ operation to the above terms. This amounts to the following objective funtion:</p>
<p>$$<br>J(\theta_G, \theta<em>D) =  min</em>{G} max<em>{D}  E</em>{x \sim p<em>{data}} [ log D(x) ] + E</em>{z \sim p_{predefined}} [ log 1 - D(G(z)) ]<br>$$</p>
<h1 id="Blind-Source-separation"><a href="#Blind-Source-separation" class="headerlink" title="Blind-Source separation"></a>Blind-Source separation</h1><p>The central question of BSS is this: Given an observation that is a mix of a number of different sources, can we recover both the underlying mechanism of such mixing and the sources, having access to the observation only?</p>
<p>In general, the answer is “no”, because the problem is too difficult to solve. But if we add some conditions on the properties of sources, then we could recover both when the mixing mechanism can be considered as a linear system.  </p>
<p>Here, we consider a non-linear version of the BSS problem. That is, given an observation $x$, we assume $x$ is generated from an unknown non-linear function $F$ with an unknown input $z$. </p>
<p>To make it suitable for our image experiment, suppose that we mix two different images, say, MNIST data and some synthesized data. We assume that these two images live in two different data manifolds. The sources $z$ here are precisely these two manifolds, and the non-linear function $F$ is the generating process of the mixture image $x$ from $z$s. The question is, when we observe only $x$, which is a mixture of two images, can we recover $F$, $z_1$ and $z_2$, (and thus recover two original images) by modeling a part of $F$ as GAN? (We don’t have access to the underlying manifold, so in our experiment the recovery of $z_1$ and $z_2$ will be done implicitly.)</p>
<h1 id="Preliminary-Model"><a href="#Preliminary-Model" class="headerlink" title="Preliminary Model"></a>Preliminary Model</h1><p>Essentially, we attempt to solve the problem by using autoencoder, where the decoder part consists of two pre-trained GANs + an additional decoder that converts the output of two GANs into one image that is an estimate of the original input, which is the mixture of the two images.    </p>
<ol>
<li>Pre-training:  Let G’(z) and G(z) are the two generative models learned by GAN’s framework using MNIST and synthesized data, respectively. </li>
<li>Build an autoencoder using G’(z) and G(z) as a part of decoder.  </li>
</ol>
<p>We can model $G’(z)$ and $G(z)$ by DCGAN. For implementation, I should be careful how to make a good alignment between the two-dimensional $z$ with the encoder. (Two-dimensional because we have two images.) </p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>(To be followed.)</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-01-12T10:15:57.000Z"><a href="/blog/2017/01/12/notes-nn-optim/">2017-01-12</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2017/01/12/notes-nn-optim/">Notes on neural network optimization</a></h1>
  

    </header>
    <div class="entry">
      
        <p><em>This is an on-going notes and subject to change. Last updated: 2017/01/13</em></p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>Optimization is inherently tied with machine learning because what “learning” means is essentially about minimizing an objective function associated with the problem of interest. In fact, the resurgence of neural network stems from various inventions that allow neural network optimization easier, faster, and better (generalization); Dropout, ReLU activation, Batch Normalization, LSTM (for RNN to avoid gradient explosion/vanishing) to name a few. </p>
<p>One of the fundamental questions with regard to optimization in deep neural network is the following: Why is finding local minima enough? Since it is virtually impossible to find analytical solutions in high dimensional non-convex optimization problems, we have to rely on iterative methods, which doesn’t necessarily gurantee to give us good solutions. The dominanting optimization algorithms in deep learning as of January 2017 are all variants of stochasitc gradient descent; Adam, AdaGrad, and RMSProp etc. However, since SGD uses only local information to update the current estimates, it is likely that the algorithm will get stuck around local minima. In practice, however, local minima found by SGD-type algorithms with appropriate hyperparameter tuning are good enough to achieve impressive results in many real tasks.  </p>
<p>It is hypothesized that the reason why local minima are good enough is that there is no “bad” local minima. That is, all local minima are very close to global minima in the error surface of deep neural network. For deep linear models, there is <a href="https://arxiv.org/abs/1605.07110" target="_blank" rel="external">a recent paper</a> that shows all local minima are global minima under some assumptions (which are not satisfied by models used in real tasks). </p>
<p>For deep neural network, it is partially backed up by Dauphin’s paper. Recent progress on statistical physics and random matrix theory show that the error surface of random Gaussian fields have interesting structure; most of local minima are close to global minima. Based on these results, Dauphin hypothesized that the error surface of neural network also follows a similar structure when the number of parameters is huge. This shows that the answer might be due to the scale of the neural network. </p>
<h3 id="Dauphin’s-arguments-with-regard-to-saddle-points"><a href="#Dauphin’s-arguments-with-regard-to-saddle-points" class="headerlink" title="Dauphin’s arguments with regard to saddle points"></a>Dauphin’s arguments with regard to saddle points</h3><p>Dauphin’s argument behind his proposed algorithm goes like this: “The reason why many algorithms seem to get stuck during training is because of saddle points surrounded by high error flat regions, which looks as if the algorithm is stuck at high error local minima. So we developed an algorithm that escapes from these high error flat regions.”  </p>
<p>I wasn’t sure how realible the first part of their statement; optimizing deep neural network gets stuck at high error surface. It might be from the views based on past research (~2011) where we didn’t have effective tools like BN and ReLU. (Indeed, their experiments only deal with deep auto-encoder.)   </p>
<p>I think as of now, we have an updated view on neural network optimization. In fact, Goodfellow &amp; Vinyals (2015) show that typical deep network are easy to optimize and can achieve near-zero loss on the training set. (What’s hard is to find the ones with good generalization error, which will be discussed in Section 3.)</p>
<p>Although Dauphin’s claim for saddle points might not exactly work for deep neural network, this saddle points hypothesis was certainly a driving force for many interesting non-convex optim papers that recently appeared outside of deep learning community. One of the impressive results is <a href="https://arxiv.org/pdf/1503.02101v1.pdf" target="_blank" rel="external">this paper</a> by Rong Ge, which provides the answer to the local v.s. global minima question in the context of Matrix Completion. The same author also shows that SGD converges to local minima in polynomial time for Tensor Factorization. Another approach is taken by John Wright, Micheal Jordan, etc, but I haven’t follow their papers too closely.  </p>
<h1 id="2-Degenerate-Hessian"><a href="#2-Degenerate-Hessian" class="headerlink" title="2. Degenerate Hessian"></a>2. Degenerate Hessian</h1><p>Most of the results to show “local minima are global minima”-type arugments assume that loss functions do not have degenerate Hessian. Degenerate Hessians are the ones that has more than one eigelvalue being 0. In deep neural network, we have degenerate Hessian everywhere, which is why we can’t simply apply the results from the above works to deep neural network.</p>
<p>(Side notes: this is also why deep learning is hard to analyze using classical statistics and decision theory framework, which heavily relies on the fact that Hessian being non-singular. This assures asymptotic normality for posterior distribution.)</p>
<p><a href="https://arxiv.org/pdf/1611.07476.pdf" target="_blank" rel="external">The recent paper</a> from Facebook discusses the consequence of degenerate Hessian specifically in deep learning, so I’ll summerize the main points in the follwing:</p>
<ul>
<li>Eigenvalue spectrum composes two parts: the bulk around 0 and the edges, where it is hypothesized that the former implies the over-parametrization of the model and the latter indicates the complexity of the input data. </li>
</ul>
<p><a href="https://arxiv.org/pdf/1611.01838v3.pdf" target="_blank" rel="external">The recent paper</a> by Chaudhari gives more details on the characteristics of the scale of the values at the edge of eigenvalue spectrum; they show that positive eigenvalues have a long tail, and negative eigenvalues have much faster decay. </p>
<p>According to Chaudhari, this trend is ubiquitous across a variety of network architectures, sizes, datasets, or optimization algorithms, which suggests that “local minima that generalize well and are discovered by gradient descent lie in “wide valleys” of energy landscape”, which are characterized by degenerate Hessians.</p>
<h1 id="3-Generalization-performance"><a href="#3-Generalization-performance" class="headerlink" title="3. Generalization performance"></a>3. Generalization performance</h1><p>One important thing we need to remember is that our goal is not to find the global minima of the objective function of interest, but to find good enough minima that has high generalization performance. We don’t want to overfit our model to the training data. </p>
<p>Motivated by the observation in the above section, Chaudhari proposed <a href="https://arxiv.org/pdf/1611.01838v3.pdf" target="_blank" rel="external">Entropy-SGD</a>, which is actively seeking flat regions (with low error), as opposed to Dauphin’s algorithm, which intentionally escapes from saddle points. </p>
<p>[details of Entropy-SGD will be followed.]</p>
<h3 id="Batch-Normalization-and-Generalization-Performance"><a href="#Batch-Normalization-and-Generalization-Performance" class="headerlink" title="Batch Normalization and Generalization Performance"></a>Batch Normalization and Generalization Performance</h3><p><a href="https://arxiv.org/abs/1511.06747" target="_blank" rel="external">This paper</a> provides a unified framework that generalizes Batch Normalization and other methods (path-normalized approach).<br>Is there theoretical argument as to the generalization power of Batch Normalization? I often heard the phrase like: “If you use BN, you don’t need to use Dropout.” This seems to be problem-dependent at least for me. Can we say something like, Batch Normalization allows the network to converge to flat regions with low error (and thus high generalization performance)?</p>
<h1 id="4-1st-order-v-s-2nd-order"><a href="#4-1st-order-v-s-2nd-order" class="headerlink" title="4. 1st order v.s. 2nd order"></a>4. 1st order v.s. 2nd order</h1><p>If we could implement Natural Gradient Descent in a large-scale setting, then it’s ideal for good generalization performance and invariance properties.<br>(For details, see <a href="https://papers.nips.cc/paper/3234-topmoumoute-online-natural-gradient-algorithm.pdf" target="_blank" rel="external">this paper</a></p>
<p>The issue is that we can’t really work with Natural Gradient because computing full Fisher information matrix is prohibitive, and at present it seems that the optimization algorithms based on approximation of Fisher information matrix are not working well enough to be competitive with Adam or tuned SGD, etc, which is why 2nd-order methods are active research area. Essentially, the matrix multiplied by the gradient vector in the GD update equation in the 2nd-order method can be considered as an approximation to Fisher information matrix, so the goal of 2nd-order methods research is (in a way) to come up with a computationally feasible method to approximate Fisher information matrix.  </p>
<p>Martens and Pascanu have many interesting works on 2nd-order methods in deep neural network optimization. If you are interested in 2nd order methods, I highly recommend reading James Martens PhD thesis; it is a very good introduction and overview of the field. </p>
<p>Below, I’ll list some of the important papers:</p>
<ul>
<li><a href="http://www1.icsi.berkeley.edu/~vinyals/Files/vinyals_aistats12.pdf" target="_blank" rel="external">Krylov Subspace Descent in Deep Learning by Vinyals</a> (2011)</li>
<li><a href="http://icml2010.haifa.il.ibm.com/papers/458.pdf" target="_blank" rel="external">Hessian-Free optimization by Martens</a> (2011)</li>
<li><a href="https://arxiv.org/pdf/1206.6464.pdf" target="_blank" rel="external">Estimating the Hessian by Back-propagating Curvature by Martens</a>  </li>
<li><a href="https://arxiv.org/pdf/1503.05671.pdf" target="_blank" rel="external">K-FAC algorithm by Martens and Grosse</a>: New 2nd order method that attempts to mimic Natural Gradient by constructing an invertible approximation of Fisher information matrix in an online fashion. (2016) </li>
</ul>
<p>I’ll cite some of the comments from the following Reddit thread with regards to why L-BFGS is not used in deep learning area very often:<br><a href="https://www.reddit.com/r/MachineLearning/comments/4bys6n/lbfgs_and_neural_nets/" target="_blank" rel="external">https://www.reddit.com/r/MachineLearning/comments/4bys6n/lbfgs_and_neural_nets/</a></p>
<hr>
<p>“Back in 2011 when that paper was published, deep learning honestly didn’t work all that well on many real tasks.<br>One of the hypotheses at the time (which has since been shown to be false) is the optimization<br>problem that neural nets posed was simply too hard – neural nets are non-convex, and<br>we didn’t have much good theory at the time to show that learning with them was possible.<br>That’s one of the reasons why people started exploring different optimization algorithms for neural nets, which was a trend that continued roughly until the breakthrough results in 2012, which worked remarkably well despite only using SGD + momentum. Since then, more theory has been developed supporting this, and other tricks have been developed (BatchNorm, RMSProp/Adagrad/Adam/Adadelta) that make learning easier.”</p>
<hr>
<h1 id="Future-Directions-Optimization-for-Two-Player-game"><a href="#Future-Directions-Optimization-for-Two-Player-game" class="headerlink" title="Future Directions: Optimization for Two-Player game"></a>Future Directions: Optimization for Two-Player game</h1><p>I think Plateau-finding type alogirhtms will keep appearing in 2017. How to characterize plateau efficiently will be a key. As for the 2nd-order method, how to construct non-diagonal scaling (that will be applied to gradient vector) with low per-iteration cost is a main challenge. (Notes to myself: can we use an idea from the hessian-sketch paper to construct something like Fisher-sketch, an approximation to (invertible) Fisher information matrix? ) </p>
<p>Until now, we only talk about optimizing one objective function.<br>Generative Adversarial Network is a relatively new generative model that uses a two-player learning framework for high dimensional density estimation.<br>Although it already produces impressive results in generating images, there are several issues. One such problem is that training GAN is notroiusly hard due to the two-player nature; optimizing one function is not necessary an optimal update for the other function. Effective training scheme for GAN is certainly open for future research. </p>
<p>[Things to add: # Small minibatches are better for generalization]<br>[I should write a post for each algorithm/paper that appears in this post and just add a link for each so that this post can be more concise / easier to mantain?] </p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-10-17T00:46:43.000Z"><a href="/blog/2016/10/17/learningtolearn-1/">2016-10-17</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/10/17/learningtolearn-1/">Learning-To-Learn: RNN-based optimization</a></h1>
  

    </header>
    <div class="entry">
      
        <p>Around the middle of June, this paper came up: <a href="https://arxiv.org/pdf/1606.04474v1.pdf" target="_blank" rel="external">Learning to learn by gradient descent by gradient descent</a>. For someone who’s interested in optimization and neural network, I think this paper is particularly interesting. The main idea is to use neural network to tune the learning rate for gradient descent.</p>
<h2 id="Summary-of-the-paper"><a href="#Summary-of-the-paper" class="headerlink" title="Summary of the paper"></a>Summary of the paper</h2><p>Usually, when we want to design learning algorithms for an arbitrary problem, we first analyize the problem, and use the insight from the problem to design learning algorithms. This paper takes a one-level-above approach to algorithm design by considering a class of optimization problems, instead of focusing on one particular optimization problem. </p>
<p>The question is how to learn an optimization algorithm that works on a “class” of optimization problems. The answer is by parameterizing the optimizer. This way, we effectively cast algorithm design as a learning problem, in which we want to learn the parameters of our oprimizer (, which we call the optimzee parameters.) </p>
<p>But how do we model the optimizer? We use Recurrent Neural Network. Therefore, the parameters of the oprimizer are just the parameters of RNN. The parameters of the original function in question (i.e. the cost function of “one instance” of a problem that is drawn from a class of optimization problems) are referred as “optimizee parameters”, and are updated using the output of our optimizer, just as we update parameters using the gradient in SGD. The final optimzee parameters $\theta^*$ will be a function of the optimizer parameters and the function in question. In summary:</p>
<p>$$\theta^* (\phi, f) \text{: the final optimzee parameters}$$ </p>
<p>$$\phi \text{: the optimizer parameters}$$ </p>
<p>$$ f\text{: the function in question} $$</p>
<p>$$<br>\theta_{t+1} = \theta_t + g_t(\nabla f(\theta_t), \phi)  \text{: the update equation of the optimzee parameters}<br>$$<br>where $g_t$ is modeled by RNN. So $\phi$is the parameter of RNN. Because LSTM is better than vanilla RNN in general (citation needed*), the paper uses LSTM. Regular gradient descent algorithms use $g_t(\nabla f(\theta_t), \phi) = -\alpha \nabla f(\theta_t)$. </p>
<p>RNN is a function of the current hidden state $h_t$, the current gradient $\nabla f(\theta_t)$, and the current parameter $\phi$.</p>
<p>The “goodness” of our optimizer can be measured by the expected loss over the distribution of a function $f$, which is</p>
<p>$$ L(\phi) = \mathbb{E}_f [f(\theta^* (\phi, f))] $$</p>
<p>(I’m ignoring $w_t$ in the above expression of $L(\phi)$ because in the paper they set $w_t = 1$.)</p>
<p>For example, suppose we have a function like $f(\theta) = a \theta^2 + b\theta + c$. If $a,b,c$ are drawn from the Gaussian distribution with some fixed value of $\mu$ and $\sigma$, the distribution of the function $f$ can be defined. (Here, the class of optimization problem is a function where $a,b,c$ are drawn from Gaussian.) In this example, the optimzee parameter is $\theta$. The optimizer (i.e. RNN) will be trained by optimizing functions which are randomly drawn from the function distribution, and we want to find the best parameter $\theta$. If we want to know how good our optimizer is, we can just take the expected value of $f$ to evaluate the goodness, and use gradient descent to optimize this $L(\phi)$.</p>
<p>After understanding the above basics, all that is left is some implementation/architecture details for computational efficieny and learning capability. </p>
<p>(By the way, there is a typo in page 3 under Equation 3; <span>$\nabla_{\theta} h(\theta)$</span><!-- Has MathJax --> should be <span>$\nabla_{\theta} f(\theta)$</span><!-- Has MathJax -->. Otherwise it doesn’t make sense.)</p>
<h3 id="Coordinatewise-LSTM-optimizer"><a href="#Coordinatewise-LSTM-optimizer" class="headerlink" title="Coordinatewise LSTM optimizer"></a>Coordinatewise LSTM optimizer</h3><img src="/blog/2016/10/17/learningtolearn-1/compgraph.png" alt="title" title="title">
<p>[The Figure is from the paper : Figure 2 on page 4]</p>
<p>To make the learning problem computationally tractable, we update the optimzee parameters $\theta$ coordinatewise, much like other successful optimization methods such as Adam, RMSprop, and AdaGrad. </p>
<p>To this end, we create $n$ LSTM cells, where $n$ is the number of dimensions of the parameter of the objective function. We setup the architecture so that the parameters for LSTM cells are shared, but each has a different hidden state. This can be achieved by the code below: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">lstm = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(number_of_coordinates):</div><div class="line">    cell_list[i] = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_layers) <span class="comment"># num_layers = 2 according to the paper.</span></div></pre></td></tr></table></figure>
<h3 id="Information-sharing-between-coordinates"><a href="#Information-sharing-between-coordinates" class="headerlink" title="Information sharing between coordinates"></a>Information sharing between coordinates</h3><p>The coordinatewise architecture above treats each dimension independently, which ignore the effect of the correlations between coordinates. To address this issue, the paper introduces more sophisticated methods. The following two models allow different LSTM cells to communicate each other. </p>
<ol>
<li>Global averaging cells: a subset of cells are used to take the average and outputs that value for each cell.</li>
<li>NTM-BFGS optimizer: More sophisticated version of 1., with the external memory that is shared between coordinates.</li>
</ol>
<h2 id="Implementation-Notes"><a href="#Implementation-Notes" class="headerlink" title="Implementation Notes"></a>Implementation Notes</h2><h3 id="Quadratic-function-3-1-in-the-paper"><a href="#Quadratic-function-3-1-in-the-paper" class="headerlink" title="Quadratic function (3.1 in the paper)"></a>Quadratic function (3.1 in the paper)</h3><p>Let’s say the objective funtion is $f(\theta) = || W \theta - y ||^2$, where the elements of $W$ and $y$ are drawn from the Gaussian distribution.</p>
<p>$g$ (as in $\theta_{t+1} = \theta_t + g$) has to be the same size as the parameter size. So, it will be something like:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">g, state = lstm(input_t, hidden_state) <span class="comment"># here, input_t is the gradient of a hidden state at time t w.r.t. the hidden</span></div></pre></td></tr></table></figure>
<p>And the update equation will be:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">param = param + g</div></pre></td></tr></table></figure>
<p>The objective function is:<br><span>$$L(\phi) = \mathbb{E}_f [ \sum_{t=1}^T w_t f(\theta_t) ]$$</span><!-- Has MathJax --><br><span>$$\text{where,  }\theta_{t+1} = \theta_t + g_t$$</span><!-- Has MathJax --><br><span>$$\left[
    \begin{array}{c}
      g_t \\
      h_{t+1}  
    \end{array}
\right]
= RNN(\nabla_t, h_t, \phi)$$</span><!-- Has MathJax --></p>
<p>The loss $L(\phi)$ can be computed by double-for loop. For each loop, a different function is randomly sampled from a distribution of $f$. Then, $\theta_t$ will be computed by the above update equation. So, overall, what we need to implement is the two-layer coordinate-wise LSTM cell. The actual implementation is <a href="https://github.com/runopti/Learning-To-Learn" target="_blank" rel="external">here</a>.  </p>
<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><img src="/blog/2016/10/17/learningtolearn-1/output_38_1.png" alt="title" title="title">
<p>I compared the result with SGD, but SGD tends to work better than our optimizer for now. Need more improvements on the optimization…</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-09-04T05:21:09.000Z"><a href="/blog/2016/09/04/induced-norm/">2016-09-04</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/09/04/induced-norm/">Induced Matrix Norm</a></h1>
  

    </header>
    <div class="entry">
      
        <p>Notes on Induced Matrix Norm. I think the name comes from that fact that it is “induced” by a vector.   </p>
<p><strong>Definition:</strong><br>For a $m$ by $n$ matrix $A$, and a n-dimensional vector $x$,<br><span>$$|| A ||_p = \sup \{ ||Ax||_p : ||x||_p = 1\}$$</span><!-- Has MathJax --></p>
<p>Note that since given $A$, $ ||Ax||_p$ is a function of $x$, which is closed and bounded as $||x||_p = 1$, $||Ax||_p$ achieves maximum or minimum on some $x$. So we can replace $\sup$ with $\max$. </p>
<p><strong>Claim I : $p = 1$:</strong><br><span>$$||A||_1 = \max_{1 \le j \le n} \sum_{i=1}^m a_{ij}$$</span><!-- Has MathJax --></p>
<hr>
<p>First, let us find some upper bound on $||Ax||_1$.<br><span>$$||Ax||_1 =  || \sum_{i=1}^n A_i x_i ||_1 \le \sum_{i=1}^n |x_i| || A_i ||_1 
\le \max_j ||A_j|| \sum_{i=1}^n |x_i| = \max_j ||A_j|| ||x||_1$$</span><!-- Has MathJax --></p>
<p>So given A, we have found a constant $K = \max_j ||A_j||$ that will upper bound $||Ax||_1$ by  $K||x||_1$:<br><span>$$||Ax||_1 \le  K ||x||_1$$</span><!-- Has MathJax --></p>
<p>Next, we show that there exists $x$ for which we have equality for the above equation. Since $K = \max_j ||A_j||$, we can just let $x = [0, 0, …, 1, .. 0, 0]$ where 1 is at the $k$th position. $k$ is the index that satisfies $||A_k|| = \max_j ||A_j||$.    </p>
<p>So we’ve found that ||A||_1 = \max_j ||A_j|| . </p>
<p><strong>Claim II : $p = \infty$:</strong><br><span>$$||A||_{\infty} = \max_{1 \le i \le m} \sum_{j=1}^n a_{ij}$$</span><!-- Has MathJax --></p>
<hr>
<p>First, consider <span>$|| Ax ||_{\infty}$</span><!-- Has MathJax -->. Then,</p>
<span>$$\| Ax \|_{\infty} = 
\left \|
 \begin{matrix}
  \sum_{j=1}^n a_{1j} x_j  \\
  \sum_{j=1}^n a_{2j} x_j  \\
  \vdots \\
  \sum_{j=1}^n a_{mj} x_j
 \end{matrix}
\right \|_{\infty} = \max_i \left | \sum_{j=1}^n a_{ij} x_j \right | 
\le \max_i \sum_{j=1}^n \left | a_{ij} x_j \right | 
\le \max_i \sum_{j=1}^n \left | a_{ij} \right | \max_k \left | x_k \right |
= \max_i \left( \sum_{j=1}^n \left | a_{ij} \right | \right) \| x \|_{\infty}$$</span><!-- Has MathJax --> 
<p>So given $A$, we have found a constant <span>$K = \max_i \left( \sum_{j=1}^n \left | a_{ij} \right | \right)$</span><!-- Has MathJax --> that will upper bound <span>$||Ax||_{\infty}$</span><!-- Has MathJax -->:<br><span>$$\| Ax \|_{\infty} \le  K &nbsp;\| x \|_{\infty}$$</span><!-- Has MathJax --> </p>
<p>To find <span>$\| A \|_{\infty}$</span><!-- Has MathJax -->, we need to find some $x$ that equates the above inequality, which is fortunately straightforward as in the case of $p = 1$. By examining :</p>
<span>$$\max_i \left | \sum_k a_{ik} x_k \right | = K \max_k | x_k |$$</span><!-- Has MathJax --> 
<p>we notice that we achieve the equality if we let<br><span>$$x_k = \begin{cases}
  \frac{ a_{ik} }{ | a_{ik} |} &amp; (a_{ik} \neq 0) \\
  1  &amp;  (a_{ik} = 0)
\end{cases}$$</span><!-- Has MathJax --></p>
<p>where $k$ is the index such that <span>$A_{k,:} = \max_i \left( \sum_{j=1}^n \left | a_{ij} \right | \right)$</span><!-- Has MathJax -->  <strong><em>Note:</em></strong>  I’m using $A_k$ as denoting the $k$ th column vector of a matrix $A$, and <span>$A_{k, :}$</span><!-- Has MathJax --> as denoting the $k$ th row vector of $A$. </p>
<p><strong>Claim III : $p = 2$:</strong><br><span>$$\|A\|_2 =  \sqrt{ \lambda_{\max} }$$</span><!-- Has MathJax --><br>where $\lambda_{\max}$ is the largest eigenvalue of $A^T A$. </p>
<hr>
<p>This is the spectral norm! </p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://en.wikipedia.org/wiki/Matrix_norm" target="_blank" rel="external">https://en.wikipedia.org/wiki/Matrix_norm</a></li>
<li><a href="http://www.ece.uah.edu/courses/ee448/chapter4.pdf" target="_blank" rel="external">http://www.ece.uah.edu/courses/ee448/chapter4.pdf</a></li>
</ul>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-09-04T03:20:55.000Z"><a href="/blog/2016/09/04/hello-world/">2016-09-04</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/09/04/hello-world/">Hello World</a></h1>
  

    </header>
    <div class="entry">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-08-12T01:45:14.000Z"><a href="/blog/2016/08/12/NNprml/">2016-08-12</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/08/12/NNprml/">Neural Network Practice Problem: 5.25</a></h1>
  

    </header>
    <div class="entry">
      
        <p>I decided to do all the practice problems in Chapter 5 of <em>Pattern Recognition and Machine Learning</em> by Bishop. This chapter is about neural nets. Although the material per se is a little bit old, all the fundamentals of neural network are very well explained in here, so even today, when a bunch of new papers on deep neural network come out on arxiv almost everyday, it’s worth reading in my opinion. </p>
<p>So today I did 5.25. The problem reads:</p>
<hr>
<p>Consider a quadratic error function of the form $ E = E_0 + (w - w^{\ast} )^T H (w - w^{\ast}) $, where $w^{\ast}$ represents the minimum, and the Hessian matrix $H$ is positive definite and constant. Suppose the initial weight vector $w^{(0)}$ is chosen to be at the origin and is updated using simple gradient descent<br>$$<br>w^{(\tau)} = w^{(\tau−1)} − \rho \nabla E<br>$$<br>where $\tau$ denotes the step number, and $\rho$ is the learning rate (which is assumed to be small). Show that, after $\tau$ steps, the components of the weight vector parallel to the eigenvectors of $H$ can be written<br>$$<br>w_j^{(\tau)} = (1 − (1 − \rho \lambda_j )^{\tau} ) w_j^{\ast}<br>$$<br>where $w_j = w^Tu_j$ , and $u_j$ and $\lambda_j$ are the eigenvectors and eigenvalues, respectively, of $H$. Show that as $\tau \rightarrow \infty$, this gives $w^{(\tau)} \rightarrow w^*$ as expected, provided $|1 − \rho \lambda_j | &lt; 1$.</p>
<hr>
<p>I’m assuming that since $H$ is a constant, it is evaluated at $w^{\ast}$, and $E_0 = E(w^{\ast})$. </p>
<p>First, since $H$ is a symmetric matrix, we can say that the eigenvalues are all real, and the eigenvectors are orthogonal to each other. </p>
<p>WLOG, we assume the $u_i$s are orthonormal. So the $u_i$s form the orthonormal basis, meaning we can express any vector with these basis vectors. This means that we can express $w - w^{\ast} = \sum_i^n v_i u_i$, where $v_i$s are appropriate coefficients. We can rewrite the equation using matrix form, which is $w - w^{\ast} = U v \iff v = U^T (w - w^{\ast})$, where $U$’s columns are $u_i$s. This expression gives us new perspective: we can view the weight vector $w$ in the original coordinate with a different coordinate, which is obtained by moving $w^{\ast}$ to the origin and rotate the original coordinate by the rotation matrix $U$. </p>
<p>Let’s plug this into the given error function E. We get<br>$$<br>E = E_0 + \frac{1}{2} (Uv)^T H (Uv) = E_0 +\frac{1}{2} v^T U^T H U v<br>$$</p>
<p>Note that $H$ is a symmetric, meaning it is diagonalizable. So we get<br>$$<br>E = E_0 + \frac{1}{2} v^T U^T UDU^T U v<br>$$</p>
<p>Since U is an orthonormal matrix, we have this identity $U^T = U^{-1}$ so everything cancels out, yielding $E = E_0 + \frac{1}{2} v^T D v$. </p>
<p>Note that $E$ is a function of $w$: $E(w)$. What if we look at it from the new coordinate? We see that $E_0(w^{\ast})$ should be 0 because in the new coordinate $w^{\ast}$ is the origin. Then, we have<br>$$<br>E(v) = \frac{1}{2} v^T D v = \frac{1}{2} \sum_i^n \lambda_i v_i^2<br>$$<br>(Note that D is a diagonal matrix with diagonal elements being eigenvalues.)</p>
<p>Then,<br>$$<br>\nabla E_v(v) = D v =  \sum_i^n \lambda_i v_i<br>$$<br>So, the update equation becomes<br>$$<br>v^{(\tau)} = v^{(\tau-1)} - \rho D v^{(\tau-1)} = (I - \rho D) v^{(\tau - 1)}<br>$$</p>
<p>If we look at the last equation coordinate wise, we get $v_i^{(\tau)} =  (1 - \rho \lambda_i) v_i^{(\tau - 1)} $, so by recursion, we get<br>$$<br>v_i^{(\tau)} =  (1 - \rho \lambda_i)^{\tau} v_i^{(0)}<br>$$<br>Now, let’s go back to the original coordinate, and we get<br>$$<br>w_i^{(\tau)} - w_i^{\ast} = (1 - \rho \lambda_i)^{\tau} w_i^{(0)} - w_i^{\ast}<br>$$<br>Since w^{(0)} is the origin, the left term is 0 and moving $w_i^{\ast}$ to left, we get<br>$$<br>w_i^{(\tau)} = (1 - (1 - \rho \lambda_i)^{\tau})w_i^{\ast}<br>$$<br>, which is what we wanted to show. </p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-26T09:19:58.000Z"><a href="/blog/2016/07/26/MatrixFactorizationNotes/">2016-07-26</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/07/26/MatrixFactorizationNotes/">Matrix Factorization Notes</a></h1>
  

    </header>
    <div class="entry">
      
        <h2 id="LU-decomposition"><a href="#LU-decomposition" class="headerlink" title="LU decomposition"></a>LU decomposition</h2><p>Thm 1:<br>N-rank regular matrix A has a unique factorization LU, where L is a N-rank regular matrix and U is a N-rank upper-triangular matrix. </p>
<p>When we think about the signs of eigenvalues in neural network literature, we only deal with Hessian, which is symmetric, so the signs of all eigenvalues uniquely determine the positive definiteness of the Hessian matrix. But this is not generally true for non-symmetric positive definite matrix. </p>
<p>One example of equivalent condition of a matrix being positive definite is the existence of a unique lower triangular matrix L with real and strictly positive diagonal entries s.t. M = LL<em> holds. (M = LL</em> is called Cholesky decomposition.)</p>
<h2 id="Cholesky-decomposition"><a href="#Cholesky-decomposition" class="headerlink" title="Cholesky decomposition"></a>Cholesky decomposition</h2><p>Thm 2:<br>For a symmetric positive definite matrix M, there exists a decomposition s.t. M = LL^T</p>
<p>Pf. (sketch)<br><span>$$M = PDP^T \\
    = PSS^TP^T \\
    = B^TB \quad (B = (PS)^T) \\
    = (QR)^T QR \quad (B = QR) \\
    = R^T Q^{-1} Q R
    = R^T R 
    = LL^T$$</span><!-- Has MathJax --></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-07-07T12:19:58.000Z"><a href="/blog/2016/07/07/HessianComp/">2016-07-07</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/07/07/HessianComp/">Hessian Computation using TensorFlow</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="Compute-Hessian-using-TensorFlow"><a href="#Compute-Hessian-using-TensorFlow" class="headerlink" title="Compute Hessian using TensorFlow"></a>Compute Hessian using TensorFlow</h1><p>Reference: </p>
<ul>
<li>TensorFlow implementation of K-means algorithm: <a href="https://codesachin.wordpress.com/2015/11/14/k-means-clustering-with-tensorflow/" target="_blank" rel="external">https://codesachin.wordpress.com/2015/11/14/k-means-clustering-with-tensorflow/</a></li>
<li>Calculating Hessian in Theano (application for Newton’s method): <a href="https://groups.google.com/forum/#!topic/theano-users/2c15kq68lp8" target="_blank" rel="external">https://groups.google.com/forum/#!topic/theano-users/2c15kq68lp8</a></li>
</ul>
<p>TensorFlow has a function called tf.gradients() that computes gradient. In the past, I’ve tried to compute Hessian of an neural network objective function in Torch7 using <a href="https://github.com/twitter/torch-autograd" target="_blank" rel="external">torch-autograd</a> but it was somewhat cumbersome; there wasn’t an easy way to store/reshape parameters because Lua uses table for everything. Today, I’d like to do the same thing in TensorFlow. It should be much easier than in Torch7 due to the symbolic differentiation. </p>
<h3 id="Example-1-Quadratic-function"><a href="#Example-1-Quadratic-function" class="headerlink" title="Example 1 : Quadratic function"></a>Example 1 : Quadratic function</h3><p>We are going to use $f(x) = \frac{1}{2} x^T A x + b^T x + c$ as our first example to compute Hessian. When A is a symmetric matrix, the hessian of $f$ should be equal to $A$.</p>
<p>For simplicity, let us start with:<br><span>$$A = \left[
  \begin{array}{rrr}
    2 &amp; 2 &amp; 2 \\
    2 &amp; 2 &amp; 2 \\
    2 &amp; 2 &amp; 2
  \end{array}
\right]
\quad
b = \left[
  \begin{array}{rrr}
    3  \\
    3  \\
    3 
  \end{array}
\right]
\quad
c = 1$$</span><!-- Has MathJax --></p>
<p>The code below calculates the hessian for f(x).  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> math</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHessian</span><span class="params">(dim)</span>:</span></div><div class="line">    <span class="comment"># Each time getHessian is called, we create a new graph so that the default graph (which exists a priori) won't be filled with old ops.</span></div><div class="line">    g = tf.Graph()</div><div class="line">    <span class="keyword">with</span> g.as_default():</div><div class="line">        <span class="comment"># First create placeholders for inputs: A, b, and c.</span></div><div class="line">        A = tf.placeholder(tf.float32, shape=[dim, dim])</div><div class="line">        b = tf.placeholder(tf.float32, shape=[dim, <span class="number">1</span>])</div><div class="line">        c = tf.placeholder(tf.float32, shape=[<span class="number">1</span>])</div><div class="line"></div><div class="line">        <span class="comment"># Define our variable</span></div><div class="line">        x = tf.Variable(np.float32(np.repeat(<span class="number">1</span>,dim).reshape(dim,<span class="number">1</span>)))</div><div class="line"></div><div class="line">        <span class="comment"># Construct the computational graph for quadratic function: f(x) = 1/2 * x^t A x + b^t x + c</span></div><div class="line">        fx = <span class="number">0.5</span> * tf.matmul(tf.matmul(tf.transpose(x), A), x) + tf.matmul(tf.transpose(b), x) + c</div><div class="line">        </div><div class="line">        <span class="comment"># Get gradients of fx with repect to x</span></div><div class="line">        dfx = tf.gradients(fx, x)[<span class="number">0</span>]</div><div class="line">        <span class="comment"># Compute hessian</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(dim):</div><div class="line">            <span class="comment"># Take the i th value of the gradient vector dfx </span></div><div class="line">            <span class="comment"># tf.slice: https://www.tensorflow.org/versions/0.6.0/api_docs/python/array_ops.html#slice</span></div><div class="line">            dfx_i = tf.slice(dfx, begin=[i,<span class="number">0</span>] , size=[<span class="number">1</span>,<span class="number">1</span>])</div><div class="line">            <span class="comment"># Feed it to tf.gradients to compute the second derivative. </span></div><div class="line">            <span class="comment"># Since x is a vector and dfx_i is a scalar, this will return a vector : [d(dfx_i) / dx_i , ... , d(dfx_n) / dx_n]</span></div><div class="line">            ddfx_i = tf.gradients(dfx_i, x)[<span class="number">0</span>] <span class="comment"># whenever we use tf.gradients, make sure you get the actual tensors by putting [0] at the end</span></div><div class="line">            <span class="keyword">if</span> i == <span class="number">0</span>: hess = ddfx_i</div><div class="line">            <span class="keyword">else</span>: hess = tf.concat(<span class="number">1</span>, [hess, ddfx_i]) </div><div class="line">            <span class="comment">## Instead of doing this, you can just append each element to a list, and then do tf.pack(list_object) to get the hessian matrix too.</span></div><div class="line">            <span class="comment">## I'll use this alternative in the second example.  </span></div><div class="line"></div><div class="line">        <span class="comment"># Before we execute the graph, we need to initialize all the variables we defined</span></div><div class="line">        init_op = tf.initialize_all_variables()</div><div class="line">    </div><div class="line">        <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">            sess.run(init_op)</div><div class="line">            <span class="comment"># We need to feed actual values into the computational graph that we created above. </span></div><div class="line">            feed_dict = &#123;A: np.float32(np.repeat(<span class="number">2</span>,dim*dim).reshape(dim,dim)), b: np.float32(np.repeat(<span class="number">3</span>,dim).reshape(dim,<span class="number">1</span>)) , c: [<span class="number">1</span>]&#125;</div><div class="line">            <span class="comment"># sess.run() executes the graph. Here, "hess" will be calculated with the values in "feed_dict".</span></div><div class="line">            print(sess.run(hess, feed_dict))</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">getHessian(<span class="number">3</span>)</div></pre></td></tr></table></figure>
<pre><code>[[ 2.  2.  2.]
 [ 2.  2.  2.]
 [ 2.  2.  2.]]
</code></pre><p>We can see that the result of sess.run(hess, feed_dict) is indeed the desired value: A</p>
<h3 id="Example-2-Multilayer-Perceptron"><a href="#Example-2-Multilayer-Perceptron" class="headerlink" title="Example 2 : Multilayer Perceptron"></a>Example 2 : Multilayer Perceptron</h3><p>Next, we’ll try a small neural network model: Multilayer perceptron. We need to modify our getHessian function a little bit; we need to create one-long vector for parameters, and then slice them according to the model architecture. Otherwise tf.gradients() cannot calculate the hessian matrix.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHessianMLP</span><span class="params">(n_input, n_hidden, n_output)</span>:</span></div><div class="line">    batch_size = <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="comment"># Each time getHessianMLP is called, we create a new graph so that the default graph (which exists a priori) won't be filled with old ops.</span></div><div class="line">    g = tf.Graph()</div><div class="line">    <span class="keyword">with</span> g.as_default():</div><div class="line">        <span class="comment"># First create placeholders for inputs and targets: x_input, y_target</span></div><div class="line">        x_input = tf.placeholder(tf.float32, shape=[batch_size, n_input])</div><div class="line">        y_target = tf.placeholder(tf.float32, shape=[batch_size, n_output])</div><div class="line">    </div><div class="line">        <span class="comment"># Start constructing a computational graph for multilayer perceptron</span></div><div class="line">        <span class="comment">###  Since we want to store parameters as one long vector, we first define our parameters as below and then</span></div><div class="line">        <span class="comment">### reshape it later according to each layer specification.</span></div><div class="line">        parameters = tf.Variable(tf.concat(<span class="number">0</span>, [tf.truncated_normal([n_input * n_hidden, <span class="number">1</span>]), tf.zeros([n_hidden, <span class="number">1</span>]),</div><div class="line">                                                                                                      tf.truncated_normal([n_hidden * n_output,<span class="number">1</span>]), tf.zeros([n_output, <span class="number">1</span>])]))</div><div class="line">        </div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"hidden"</span>) <span class="keyword">as</span> scope:</div><div class="line">            idx_from = <span class="number">0</span> </div><div class="line">            weights = tf.reshape(tf.slice(parameters, begin=[idx_from, <span class="number">0</span>], size=[n_input*n_hidden, <span class="number">1</span>]), [n_input, n_hidden])</div><div class="line">            idx_from = idx_from + n_input*n_hidden</div><div class="line">            biases = tf.reshape(tf.slice(parameters, begin=[idx_from, <span class="number">0</span>], size=[n_hidden, <span class="number">1</span>]), [n_hidden]) <span class="comment"># tf.Variable(tf.truncated_normal([n_hidden]))</span></div><div class="line">            hidden = tf.matmul(x_input, weights) + biases</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">"linear"</span>) <span class="keyword">as</span> scope:</div><div class="line">            idx_from = idx_from + n_hidden</div><div class="line">            weights = tf.reshape(tf.slice(parameters, begin=[idx_from, <span class="number">0</span>], size=[n_hidden*n_output, <span class="number">1</span>]), [n_hidden, n_output])</div><div class="line">            idx_from = idx_from + n_hidden*n_output</div><div class="line">            biases = tf.reshape(tf.slice(parameters, begin=[idx_from, <span class="number">0</span>], size=[n_output, <span class="number">1</span>]), [n_output]) </div><div class="line">            output = tf.nn.softmax(tf.matmul(hidden, weights) + biases)</div><div class="line">        <span class="comment"># Define cross entropy loss</span></div><div class="line">        loss = -tf.reduce_sum(y_target * tf.log(output))</div><div class="line">        </div><div class="line">        </div><div class="line">        <span class="comment">### Note: We can call tf.trainable_variables to get GraphKeys.TRAINABLE_VARIABLES </span></div><div class="line">        <span class="comment">### because we are using g as our default graph inside the "with" scope. </span></div><div class="line">        <span class="comment"># Get trainable variables</span></div><div class="line">        tvars = tf.trainable_variables()</div><div class="line">        <span class="comment"># Get gradients of loss with repect to parameters</span></div><div class="line">        dloss_dw = tf.gradients(loss, tvars)[<span class="number">0</span>]</div><div class="line">        dim, _ = dloss_dw.get_shape()</div><div class="line">        hess = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(dim):</div><div class="line">            <span class="comment"># tf.slice: https://www.tensorflow.org/versions/0.6.0/api_docs/python/array_ops.html#slice</span></div><div class="line">            dfx_i = tf.slice(dloss_dw, begin=[i,<span class="number">0</span>] , size=[<span class="number">1</span>,<span class="number">1</span>])</div><div class="line">            ddfx_i = tf.gradients(dfx_i, parameters)[<span class="number">0</span>] <span class="comment"># whenever we use tf.gradients, make sure you get the actual tensors by putting [0] at the end</span></div><div class="line">            hess.append(ddfx_i)</div><div class="line">        hess = tf.squeeze(hess) </div><div class="line">        init_op = tf.initialize_all_variables()</div><div class="line">        <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">            sess.run(init_op)</div><div class="line">            feed_dict = &#123;x_input: np.random.random([batch_size, n_input]), y_target: np.random.random([batch_size, n_output])&#125;</div><div class="line">            <span class="comment">#print(sess.run(loss, feed_dict))</span></div><div class="line">            print(hess.get_shape())</div><div class="line">            print(sess.run(hess, feed_dict))</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">getHessianMLP(n_input=<span class="number">3</span>,n_hidden=<span class="number">4</span>,n_output=<span class="number">3</span>)</div></pre></td></tr></table></figure>
<pre><code>(31, 31)
[[  2.19931314e-03  -1.42659002e-03   1.30957202e-03  -8.70158256e-04
    8.50890204e-03  -5.51932165e-03   5.06659178e-03  -3.36654740e-03
    7.25943921e-03  -4.70885402e-03   4.32260381e-03  -2.87219742e-03
    1.87662840e-02  -1.21727986e-02   1.11743081e-02  -7.42488075e-03
   -5.90046346e-02   8.59218910e-02  -2.69172415e-02  -1.75508182e-03
    3.87416431e-03  -2.11908249e-03  -5.98554593e-03   1.32124824e-02
   -7.22693698e-03   3.56099289e-03  -7.86052924e-03   4.29953635e-03
   -1.33406427e-02   2.94481106e-02  -1.61074679e-02]
 [ -1.42659002e-03   2.15499196e-03   2.23391340e-03   5.85207134e-04
   -5.51932165e-03   8.33742879e-03   8.64276756e-03   2.26410269e-03
   -4.70885402e-03   7.11314566e-03   7.37364776e-03   1.93163764e-03
   -1.21727986e-02   1.83881018e-02   1.90615226e-02   4.99345176e-03
   -2.40529864e-03  -1.63234770e-02   1.87287778e-02  -5.11422716e-02
    6.40287027e-02  -1.28864162e-02  -1.72071008e-03  -1.16775408e-02
    1.33982506e-02   1.02370558e-03   6.94734277e-03  -7.97104836e-03
   -3.83513537e-03  -2.60270163e-02   2.98621524e-02] ... [omitted] ]
</code></pre>
      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-29T21:31:24.000Z"><a href="/blog/2016/06/30/MetricLearningPart1/">2016-06-30</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/06/30/MetricLearningPart1/">Metric Learning Part1</a></h1>
  

    </header>
    <div class="entry">
      
        <h2 id="Metric-Learning-Part-1"><a href="#Metric-Learning-Part-1" class="headerlink" title="Metric Learning: Part 1"></a>Metric Learning: Part 1</h2><hr>
<p><strong><em>Note:</em></strong> <em>this post is the first part of the distance metric series. The first post discusses “Distance Metric Learning, with application<br>to clustering with side-information” (<a href="http://ai.stanford.edu/~ang/papers/nips02-metric.pdf" target="_blank" rel="external">http://ai.stanford.edu/~ang/papers/nips02-metric.pdf</a>) and the<br>second post discusses “Geometric Mean Metric Learning” (<a href="http://suvrit.de/papers/GMML.pdf" target="_blank" rel="external">http://suvrit.de/papers/GMML.pdf</a>)</em></p>
<hr>
<p>I attended one of the optimization session on the first day of ICML and one of the talks I listened to was about geometric<br>mean metric learning (<a href="http://suvrit.de/papers/GMML.pdf" target="_blank" rel="external">http://suvrit.de/papers/GMML.pdf</a>). I was amazed by their result so I’ll make some notes about<br>their method for myself. </p>
<p>In many machine learning tasks (i.e. classification, search, clustering),<br>it is necessary to learn some sort of notion of distance between input<br>data points. For example, in classification, one needs to –</p>
<p>Metric learning was (first?) introduced in this paper: “Distance Metric Learning, with application<br>to clustering with side-information”<br><a href="http://ai.stanford.edu/~ang/papers/nips02-metric.pdf" target="_blank" rel="external">http://ai.stanford.edu/~ang/papers/nips02-metric.pdf</a>.</p>
<h3 id="Problem-Setting"><a href="#Problem-Setting" class="headerlink" title="Problem Setting"></a>Problem Setting</h3><hr>
<p>Suppose we have a set of points <span>$\{x_i\}_{i=1}^n \subseteq R^m$</span><!-- Has MathJax -->. Suppose also we are given a set of “side information” that tells us that a certain set of points are “similar” s.t. <span>$S: (x_i, x_j) \in S \text{ if } x_i \text{ and } x_j \text{ are similar. }$</span><!-- Has MathJax --> How can we encode this similar-dissimilar information in a distance metric? </p>
<h3 id="Simple-answer"><a href="#Simple-answer" class="headerlink" title="Simple answer"></a>Simple answer</h3><hr>
<p>Learn a Mahalanobis distance $d(x,y) = \sqrt{(x-y)^T A (x-y)}$ so that $A$ will somehow encode the desired information. </p>
<p>Now the problem is reduced to how to learn the matrix A. Whenever we want to learn something, we should remind ourselves of viewing it as optimization. To do so, we need to define what we want to minimize (or maximize…just flip the sign of the objective function), which should correspond to the notion of “goodness” of the matrix A. But before that, we will go over how the Mahalanobis distance came in.</p>
<h3 id="Mahalanobis-distance-in-R-2"><a href="#Mahalanobis-distance-in-R-2" class="headerlink" title="Mahalanobis distance in R^2"></a>Mahalanobis distance in R^2</h3><p>To get the feel of it, we will look at some examples. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line">mu = [<span class="number">0</span>, <span class="number">0</span>]</div><div class="line">cov = [[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>]]</div><div class="line">x_data, y_data = np.random.multivariate_normal(mu,cov,<span class="number">500</span>).T</div><div class="line"><span class="comment">#x_data = np.float32(np.random.rand(1,1000))</span></div><div class="line"><span class="comment">#y_data = np.float32(np.random.rand(1,1000))</span></div><div class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</div><div class="line">plt.axis([<span class="number">-10.0</span>,<span class="number">10.0</span>,<span class="number">-10.0</span>,<span class="number">10.0</span>],size=<span class="number">20</span>)</div><div class="line">plt.grid(<span class="keyword">True</span>)</div><div class="line">ax = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</div><div class="line">plot_out = plt.scatter(x_data,y_data,color=<span class="string">'b'</span>,marker=<span class="string">'o'</span>,label=<span class="string">"$K_2,mu_2$"</span>, alpha=<span class="number">0.3</span>)</div><div class="line">circle = plt.Circle((<span class="number">0</span>,<span class="number">0</span>),<span class="number">1</span>,color=<span class="string">'r'</span>, fill=<span class="keyword">False</span>, alpha=<span class="number">0.9</span>)</div><div class="line">plot_out = ax.add_artist(circle)</div><div class="line"><span class="comment">#plot_out = plt.plot(x_data, y_data, "bo", alpha=0.3, )</span></div><div class="line"></div><div class="line"><span class="comment">#plt.show()</span></div></pre></td></tr></table></figure>
<img src="/blog/2016/06/30/MetricLearningPart1/output_6_0.png" alt="title" title="title">
<p>Suppose that the data distribution were Gaussian and our data samples are distributed according to $N(0, I)$. The red circle marks the unit standard deviation. Now, take two blue points x and y. The Euclidian distance between x and y is $|| x - y ||_2 = \sqrt{(x-y)^T (x-y)}$. Now normally the data are not distributed according to Gaussian, but rather some obscure distribution. In this data distribution, some data samples might be “similar” and others are not. If we use Euclidian distance as a way to measure similarity, we might end up disregarding the intrinsic information within the data distribution. We want to “correct” this.  </p>
<p>To be more concrete, suppose our real data distribution were something like $\mu = 0$ and $Cov = [[1,0], [0, 6]], which is still too simplistic for any real data. Then sample distribution might look like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line">mu = [<span class="number">0</span>, <span class="number">0</span>]</div><div class="line">cov = [[<span class="number">3</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>]]</div><div class="line">x_data, y_data = np.random.multivariate_normal(mu,cov,<span class="number">500</span>).T</div><div class="line"><span class="comment">#x_data = np.float32(np.random.rand(1,1000))</span></div><div class="line"><span class="comment">#y_data = np.float32(np.random.rand(1,1000))</span></div><div class="line">fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</div><div class="line">plt.axis([<span class="number">-10.0</span>,<span class="number">10.0</span>,<span class="number">-10.0</span>,<span class="number">10.0</span>],size=<span class="number">20</span>)</div><div class="line">plt.grid(<span class="keyword">True</span>)</div><div class="line">ax = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>)</div><div class="line">plot_out = plt.scatter(x_data,y_data,color=<span class="string">'b'</span>,marker=<span class="string">'o'</span>,label=<span class="string">"$K_2,mu_2$"</span>, alpha=<span class="number">0.3</span>)</div><div class="line">circle = plt.Circle((<span class="number">0</span>,<span class="number">0</span>),<span class="number">1</span>,color=<span class="string">'r'</span>, fill=<span class="keyword">False</span>, alpha=<span class="number">0.9</span>)</div><div class="line">plot_out = ax.add_artist(circle)</div><div class="line"></div><div class="line"><span class="comment"># TO DO: I should fix the red circle so that it will correspond to the blue distribution.</span></div></pre></td></tr></table></figure>
<img src="/blog/2016/06/30/MetricLearningPart1/output_9_0.png" alt="title" title="title">
<p>In order to reflect the distortion, we should use ; what is the correct way to measure the distance between x and y in the new plot? For example, can we say that the point at the top of the distribution and that point right this and this are the same distance? We should fix this because we think that those two points that are along the principle direction should be “more” similar than those that are not (in this case perpendicular). How can we fix this? By rotating back. The rotation matrix we used was C. So one correct distance metric we could use is $|| x - y ||_{C^{-1}} = \sqrt{(x-y)^T C^{-1} (x-y)}$</p>
<p>Now let’s go back to the paper. What the paper says is this: why not let the data decide which matrix $C^{-1}$ to use. </p>
<h3 id="Optimization-problem"><a href="#Optimization-problem" class="headerlink" title="Optimization problem"></a>Optimization problem</h3><p>The simplest formulation would be : <span>$\min \sum_{x_i, y_i \in S} || x_i - y_i ||_A$</span><!-- Has MathJax -->. However, this would lead to the non-interesting answer which is to let A be 0 matrix, which will give us 0 for even pairs of x and y which are in a dissimilar set $D$. So we should have a restriction which prevents that to happen. One way to do is add a constraint on $A$ such that $\sum_{x_i, y_i \in D} || x_i - y_i ||_A \ge c$, where $c$ is some positive constant.  such that (most of) $|| x_i - y_i ||_A = 0$ even if $x_i \neq y_i$, which is not even a distance anymore. (Recall the definition of “distance”.) So the final form is:<br><span>$\min \sum_{x_i, y_i \in S} || x_i - y_i ||_A  \text{ s.t. }  \sum_{x_i, y_i \in D} || x_i - y_i ||_A \ge c \text{ and } A \succeq 0$</span><!-- Has MathJax --></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-06-06T09:17:33.000Z"><a href="/blog/2016/06/06/Ch19-Approximate-Inference/">2016-06-06</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/06/06/Ch19-Approximate-Inference/">Ch19: Approximate Inference</a></h1>
  

    </header>
    <div class="entry">
      
        <p>Notes for myself: <strong>Ch19</strong> <strong>Approximate Inference</strong> <a href="http://www.deeplearningbook.org/contents/inference.html" target="_blank" rel="external">http://www.deeplearningbook.org/contents/inference.html</a></p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul>
<li><p>Inference = compute $P(h|v)$.</p>
</li>
<li><p>There are many situations where you want to calculate the posterior distribution $P(h|v)$ (i.e. sparse coding, ). Here, h is a set of hidden variables and v is a set of observed variables. In general, when the model is complicated, it is intractable to compute the corresponding $P(h|v)$. This chapter introduces many examples of the inference problem, which approximates $P(h|v)$.   </p>
</li>
</ul>
<h3 id="19-1-Inference-as-Optimization"><a href="#19-1-Inference-as-Optimization" class="headerlink" title="19.1 Inference as Optimization"></a>19.1 Inference as Optimization</h3><ul>
<li><p>The core idea of approximate inference.</p>
</li>
<li><p>Given a probabilistic model with latent variables $h$ and observed variables $v$, we want to know how good it is. One measure of goodness is $\log p(v; \theta)$, often called model evidence or marginalized likelihood.</p>
</li>
<li><p>When integrating out $h$ is difficult, we instead seek for an alternative. Is there a way to lower bound $\log p(v, \theta)$?</p>
</li>
<li><p>Consider the following:<br>$$<br>L(v, \theta, q) = \log p(v; \theta) - KL(q(h|v), p(h|v; \theta))<br>$$</p>
</li>
<li><p>Since KL divergence is always non-negative, we can think of L(v, \theta, q) as a lower bound approximation of $\log p(v, \theta)$.  </p>
</li>
<li><p>By modifying the above equation, we have</p>
<span>$$L(v,\theta,q) = \log p(v;\theta) - E_{h \sim q}\log \frac{q(h|v)}{p(h|v;\theta)} \\
= \log p(v; \theta) - E_{h \sim q}\log\frac{q(h|v)}{\frac{p(v,h; \theta)}{p(v; \theta)}} \\
= \log p(v; \theta) - E_{h\sim q}[ \log q(h|v) - \log \frac{p(v,h; \theta)}{p(v; \theta)}] \\
= - E_{h\sim q}[ \log q(h|v) - \log p(v,h; \theta)] \\
= - E_{h\sim q}[ \log q(h|v)] + E_{h \sim q}[\log p(v,h; \theta)] \\
= E_{h \sim q}[\log p(v,h; \theta)] + H(q)$$</span><!-- Has MathJax -->
<p>where <span>$H(q) = - E_{h\sim q}[ \log q(h|v)]$</span><!-- Has MathJax -->.</p>
</li>
<li><p>For an appropriate choice of $q$, $L$ is tractable.</p>
</li>
<li><p>The following sections will show how to derive different forms of approximate inference by using approximate optimization to find $q$.</p>
</li>
</ul>
<h3 id="19-2-Expectation-Minimization"><a href="#19-2-Expectation-Minimization" class="headerlink" title="19.2 Expectation Minimization"></a>19.2 Expectation Minimization</h3><p>(I thought it’s easier to view this section through an example so I’ll add some Mixture-of-Gaussian taste in my notes)</p>
<ul>
<li>The goal is to maximize the lower bound $L(v,\theta, q)$. Note that we have two distinct terms when we expand $L$ in the above section: the left term is parameterized by $\theta$ and the right term is parameterized by $q$.</li>
</ul>
<h3 id="19-3-MAP-Inference-and-Sparse-Coding"><a href="#19-3-MAP-Inference-and-Sparse-Coding" class="headerlink" title="19.3 MAP Inference and Sparse Coding"></a>19.3 MAP Inference and Sparse Coding</h3><p>(will skip this for the time being)</p>
<h3 id="19-4-Variational-Inference-and-learning"><a href="#19-4-Variational-Inference-and-learning" class="headerlink" title="19.4 Variational Inference and learning"></a>19.4 Variational Inference and learning</h3><ul>
<li>The core idea: maximize $L$ over a restricted family of distributions $q$.</li>
</ul>
<h3 id="19-5-Learned-Approximate-Inference"><a href="#19-5-Learned-Approximate-Inference" class="headerlink" title="19.5 Learned Approximate Inference"></a>19.5 Learned Approximate Inference</h3><ul>
<li>The core idea: we can view the iterative optimization of maximizing $L(v,q)$ w.r.t. $q$ as a function $f$ that maps an input $v$ to an approximate distribution $q* = argmax_q L(v,q)$. Once we view this way, we can learn a function $f(v; \theta)$ with neural network.   </li>
</ul>
<h2 id="Ch14-Autoencoders"><a href="#Ch14-Autoencoders" class="headerlink" title="Ch14 : Autoencoders"></a>Ch14 : Autoencoders</h2><h3 id="14-4-Stochastic-Encoders-and-Decoders"><a href="#14-4-Stochastic-Encoders-and-Decoders" class="headerlink" title="14.4 Stochastic Encoders and Decoders"></a>14.4 Stochastic Encoders and Decoders</h3><h2 id="Ch3-Probability-and-Information-Theory"><a href="#Ch3-Probability-and-Information-Theory" class="headerlink" title="Ch3 : Probability and Information Theory"></a>Ch3 : Probability and Information Theory</h2><p><a href="http://www.deeplearningbook.org/contents/prob.html" target="_blank" rel="external">http://www.deeplearningbook.org/contents/prob.html</a></p>
<h3 id="3-13-Information-Theory"><a href="#3-13-Information-Theory" class="headerlink" title="3.13 Information Theory"></a>3.13 Information Theory</h3><p><em>Shannon entropy</em> = $H(x)$ = $-E_{x \sim P}[\log P(x)]$ (also denoted $H(P)$)</p>
<p>In words, the Shannon entropy of a distribution $P$ is the expected amount of information in an event drawn from that distribution.</p>
<p><em>KL divergence</em> = $D<em>{KL}[P || Q]$ = $E</em>{x \sim P}[\log P(x) - \log Q(x)]$</p>
<p><em>Cross entropy</em> = $H(P,Q)$ = $H(P) + D<em>{KL}(P||Q)$ = $-E</em>{x \sim P}\log Q(x)$</p>
<p>Minimizing the cross-entropy w.r.t Q is equivalent to minimizing the KL divergence.</p>
<h2 id="Ch5-Machine-Learning-Basics"><a href="#Ch5-Machine-Learning-Basics" class="headerlink" title="Ch5 : Machine Learning Basics"></a>Ch5 : Machine Learning Basics</h2><h3 id="5-5-Maximum-Likelihood-estimation"><a href="#5-5-Maximum-Likelihood-estimation" class="headerlink" title="5.5 Maximum Likelihood estimation"></a>5.5 Maximum Likelihood estimation</h3><p>-</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
  
    <a href="/blog/page/2/" class="alignright next">Nächste Seite</a>
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Suche">
    <input type="hidden" name="q" value="site:runopti.github.io/blog">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/blog/tags/PRML/">PRML</a><small>1</small></li>
  
    <li><a href="/blog/tags/cv/">cv</a><small>1</small></li>
  
    <li><a href="/blog/tags/math/">math</a><small>5</small></li>
  
    <li><a href="/blog/tags/neuralnet/">neuralnet</a><small>7</small></li>
  
    <li><a href="/blog/tags/optimization/">optimization</a><small>4</small></li>
  
    <li><a href="/blog/tags/paper-memo/">paper-memo</a><small>2</small></li>
  
    <li><a href="/blog/tags/research/">research</a><small>0</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2017 Yutaro Yamada
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/blog/js/jquery.imagesloaded.min.js"></script>
<script src="/blog/js/gallery.js"></script>




<link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/blog/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]],"processEscapes":true}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
