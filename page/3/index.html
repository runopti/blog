<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Página 3 | Notes</title>
  <meta name="author" content="Yutaro Yamada">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Notes"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/blog/favicon.png" rel="icon">
  <link rel="alternate" href="/blog/atom.xml" title="Notes" type="application/atom+xml">
  <link rel="stylesheet" href="/blog/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header id="header" class="inner"><div class="alignleft">
  <h1><a href="/blog/">Notes</a></h1>
  <h2><a href="/blog/"></a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/blog/">Home</a></li>
    
      <li><a href="/blog/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-30T06:55:14.000Z"><a href="/blog/2016/05/30/conjugate-gradient/">2016-05-30</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/05/30/conjugate-gradient/">Conjugate Gradient Method Derivation</a></h1>
  

    </header>
    <div class="entry">
      
        <ul>
<li>CG solves a linear equation $Ax = b$.</li>
</ul>
<h4 id="Derivation"><a href="#Derivation" class="headerlink" title="Derivation:"></a>Derivation:</h4><ol>
<li>Recall the step size calculation of Steepest Gradient Descent.</li>
</ol>
<span>$$-\nabla f(x_i)^T r_{i+1} = 0 \\
\iff r_i^T r_{i+1} = 0 \\
\iff (b - Ax_{i+1})^T r_{i} = 0 \\
\iff (b - A(x_i - \alpha \nabla f(x_i))^T r_i = 0 \\
\iff (r_i - \alpha A r_i)^T r_i = 0 \\
\iff \alpha = \frac{r_i^T r_i}{r_i^T A r_i}$$</span><!-- Has MathJax -->
<p>Recall that $f(x) = \frac{1}{2} x^T A x - b^Tx + c$ is the corresponding parabollic function for the linear equation $Ax = b$ (the minus sign for b is just to make it nice when we consider the relation between $Ax=b$ and its quadratic form). And $f’(x) = \frac{1}{2}A^Tx + \frac{1}{2}Ax - b$</p>
<p>When $A$ is symmetric, this becomes $f’(x) = Ax - b$. So $-f’(x_i) = r_i$, where $r_i = b - Ax_i$ (residual vector)</p>
<ol>
<li><p>Recall the typical trajectory of Steepest Descent. It is often redundant or inefficient when it’s close to the true solution. Can we improve somehow? For example, if we can pick $n$ orthogonal search directions {$d_1, d_2, … d_n$} such that we only need exactly one step for each direction to get to the solution, it would be much more efficient. One obvious choice (but impractical) is to choose coordinate axes as our search directions. This would lead us to:</p>
<ul>
<li>Update Equation: $x_{i+1} = x_i + \alpha d_i$</li>
<li>How to find $\alpha$ :<ul>
<li>Observe that $d<em>i$ has to be orthogonal to $e</em>{i+1} = x_{i+1} - x$. So,</li>
</ul>
</li>
</ul>
</li>
</ol>
<span>$$e_{i+1}^T d_i = 0 \\
\iff (x_{i+1} - x)^T d_i = 0 \\
\iff (x_{i} + \alpha d_i - x)^T d_i = 0 \\
\iff (e_{i} + \alpha d_i)^T d_i = 0 \\
\iff \alpha = - \frac{e_{i}^T d_i}{d_i^Td_i}$$</span><!-- Has MathJax -->
<ol>
<li><p>But we don’t know $e_i$ (because we don’t know $x$) so this is useless. But we can modify the above idea by picking an $A$-orthogonal set of search directions, instead of orthogonal one. $d_i$ and $d_j$ is $A$-orthogonal if $d_i^T A d_j = 0$. How can we find an appropriate step size?</p>
</li>
<li><p>Recall the original idea of step size: we want to minimize the function value most by choosing the best step size. It means we want to solve $argmin f(x_i + \alpha d_i)$. Setting the derivative w.r.t. $\alpha$ equal to 0 tells us that</p>
<span>$\frac{\partial{f(x_{i+1})}}{\partial \alpha} = 0 \iff \nabla f(x_{i+1})^T \frac{d}{d \alpha} x_{i+1} = 0 \iff -r_{i+1}^T d_i  = 0 \\$</span><!-- Has MathJax -->
</li>
</ol>
<p>(directional derivative: <a href="http://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx" target="_blank" rel="external">http://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx</a>)</p>
<p>Now recall that $r_i = b - Ax_i = b - Ax + Ax - Ax_i = -Ae_i$ (Note: b - Ax = 0). So the last term will become<br><span>$$e_{i+1}^T A d_i = 0 \iff (e_i + \alpha d_i)^T A d_i = 0 \\
\iff \alpha = -\frac{e_i^T A d_i}{d_i^T A d_i}
= -\frac{r_i^T d_i}{d_i^T A d_i}$$</span><!-- Has MathJax --></p>
<ul>
<li>So how to find an A-orthogonal set of search directions? Is the existence even guaranteed?</li>
</ul>
<h3 id="Gram-Schmidt-Conjugation"><a href="#Gram-Schmidt-Conjugation" class="headerlink" title="Gram-Schmidt Conjugation"></a>Gram-Schmidt Conjugation</h3><p><a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf" target="_blank" rel="external">https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf</a></p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-28T05:47:17.000Z"><a href="/blog/2016/05/28/duality/">2016-05-28</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/05/28/duality/">Easy way to generate a dual problem</a></h1>
  

    </header>
    <div class="entry">
      
        <p>Consider the following Primal Problem:</p>
<span>$$\min c^T x \quad 
s.t. \quad Ax \ge b, x \ge 0$$</span><!-- Has MathJax -->
<p>Then the Lagrange function will be</p>
<p>$$<br> L(x,p) = c^T x + p^T(b-Ax)<br>$$</p>
<p>If we stare at it for a while, we notice that we can rewrite the primal problem as</p>
<span>$$\min_{x\ge0} \max_{p\ge0} L(x,p)$$</span><!-- Has MathJax -->
<p>(Suppose $b-Ax\ge0$. Then by letting $p$ large, we can make $\max_{p\ge0} L(x,p)$ arbitrarily large. In order to have a sensible solution, we need $b-Ax \le 0$; the constraint is implicitly incorporated by this $max$ operator.)</p>
<p>Flip min and max, and then you get the dual problem:</p>
<span>$$\max_{p\ge0} \min_{x\ge0} L(x,p)$$</span><!-- Has MathJax -->

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-05-21T14:10:25.000Z"><a href="/blog/2016/05/21/History-of-Neural-Nets-Research-since-2006/">2016-05-21</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/05/21/History-of-Neural-Nets-Research-since-2006/">History of Neural Network Research since 2006</a></h1>
  

    </header>
    <div class="entry">
      
        <p>[On-going notes.] </p>
<h2 id="Invention-of-pre-training"><a href="#Invention-of-pre-training" class="headerlink" title="Invention of pre-training"></a>Invention of pre-training</h2><p>2006: Hinton &amp; Salakhutdinov “Reducing the Dimensionality of Data with Neural Networks”</p>
<p>2007: Bengio “Greedy layer-wise training of deep networks”</p>
<p>Pre-training resolved the issue associated with training deep networks.</p>
<p>Glorot, X. and Bengio, Y. “Understanding the difficulty of training deep feedforward neural networks”</p>
<h2 id="2nd-order-method"><a href="#2nd-order-method" class="headerlink" title="2nd order method"></a>2nd order method</h2><p>2010: Martens “Deep Learning via Hessian-Free optimization”</p>
<ul>
<li>showed that HF “is capable of training DNNs from certain random initializations without the use of pre-training, and can achieve lower errors for the various auto-encoding tasks considered (by Hinton &amp; Salakhutdinov” (Hinton 2013))</li>
</ul>
<h2 id="maybe-SGD-wasn’t-that-bad-to-train-deep-nets"><a href="#maybe-SGD-wasn’t-that-bad-to-train-deep-nets" class="headerlink" title="maybe SGD wasn’t that bad to train deep nets?"></a>maybe SGD wasn’t that bad to train deep nets?</h2><ul>
<li>Notably, Chapelle &amp; Erhan (2011) used the random initialization of Glorot &amp; Bengio (2010) and SGD to train the 11-layer autoencoder of Hinton &amp; Salakhutdinov (2006), and were able to surpass the results reported by Hinton &amp; Salakhutdinov (2006). While these results still fall short of those reported in Martens (2010) for the same tasks, they indicate that learning deep networks is not nearly as hard as was previously believed.</li>
</ul>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>2012: Hinton [“Improving neural<br>networks by preventing co-adaptation of feature detectors”]<br>(<a href="http://arxiv.org/abs/1207.0580" target="_blank" rel="external">http://arxiv.org/abs/1207.0580</a>)</p>
<h2 id="learning-rate-schedule-for-momentum"><a href="#learning-rate-schedule-for-momentum" class="headerlink" title="learning rate schedule for momentum"></a>learning rate schedule for momentum</h2><p>2013: Hinton <a href="http://www.cs.toronto.edu/~fritz/absps/momentum.pdf" target="_blank" rel="external">“On the importance of initialization and momentum in deep learning”</a></p>
<ul>
<li>when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization</li>
</ul>
<p>Interesting remark:</p>
<ul>
<li>Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.</li>
</ul>
<p>-&gt; what is the reasoning behind this?</p>
<ul>
<li><p>the optimization problem resembles an estimation one)</p>
</li>
<li><p>One explanation is that previous theoretical analyses and practical benchmarking focused on local convergence in the stochastic setting, which is more of an estimation problem than an optimization one (Bottou &amp; LeCun, 2004). In deep learning problems this final phase of learning is not nearly as long or important as the initial “transient phase” (Darken &amp; Moody, 1993), where a better argument can be made for the beneficial effects of momentum.</p>
</li>
</ul>
<p>what does this estimation-optimization thing mean?</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-01-09T05:00:00.000Z"><a href="/blog/2016/01/09/this-is-amazing/">2016-01-09</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/01/09/this-is-amazing/">This is amazing</a></h1>
  

    </header>
    <div class="entry">
      
        <p>The progress on image generation has been dramatically improved due to VAE/GAN. The quality of Figure 5 in <a href="http://arxiv.org/pdf/1512.09300.pdf" target="_blank" rel="external">this paper</a> is incredible.  </p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-01-08T05:00:00.000Z"><a href="/blog/2016/01/08/reading-deep-residual-learning-for-image-recognition/">2016-01-08</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2016/01/08/reading-deep-residual-learning-for-image-recognition/">Reading: Deep Residual Learning for Image Recognition</a></h1>
  

    </header>
    <div class="entry">
      
        <p>Links:</p>
<p><a href="http://tinyclouds.org/colorize/" target="_blank" rel="external">http://tinyclouds.org/colorize/</a></p>
<p>Notes:</p>
<ul>
<li><p>This problem, (gradient vanishing problem) however, has been largely addressed by normalized initialization [23, 9, 37, 13] and intermediate normalization layers [16], which enable networks with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation [22].  </p>
</li>
<li><p>Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error, as reported in [11, 42] and thoroughly verified by our experiments. Fig. 1 shows a typical example</p>
</li>
</ul>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-12-31T05:00:00.000Z"><a href="/blog/2015/12/31/reading-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization/">2015-12-31</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2015/12/31/reading-identifying-and-attacking-the-saddle-point-problem-in-high-dimensional-non-convex-optimization/">Reading: Identifying and attacking the saddle point problem in high dimensional non convex optimization</a></h1>
  

    </header>
    <div class="entry">
      
        <p>Important notes:</p>
<p>“Thus, given the proliferation of saddle points, not local minima, in high dimensional problems, the entire theoretical justification for quasi-Newton methods, i.e. the ability to rapidly descend to the bottom of a convex local minimum, becomes less relevant in high dimensional non-convex optimization.”</p>
<p>The proposed algorithm uses the second order curvature information in a different way than quasi-Newton methods. </p>
<p>Check:</p>
<p>“Typical, random Gaussian error functions over N scalar variables, or dimensions, are increasingly likely to have saddle points rather than local minima as N increases. Indeed the ratio of the number of saddle points to local minima increases exponentially with the dimensionality N.”</p>
<p>“Second order methods, like the Newton method, are designed to rapidly descend plateaus surroudning local minima by rescaling gradient steps by the inverse eigenvalues of the Hessian matrix.” -&gt; “H</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-03-14T04:00:00.000Z"><a href="/blog/2015/03/14/quiz1/">2015-03-14</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2015/03/14/quiz1/">nCk</a></h1>
  

    </header>
    <div class="entry">
      
        <p>I read an article about <a href="http://en.wikipedia.org/wiki/Viterbi_algorithm" target="_blank" rel="external">Viterbi algorithm</a> on wikipedia today, which (somehow) reminded me of interesting combinatorics problems back in the day. So I will share some of my favorite ones.</p>
<pre><code>Give an intuitively understandable description to the following combinatorics expressions:
</code></pre><span>$$1. _n C _k = _{n-1}C _{k} + _{n-1} C _{k-1} \\

2. k_n C _k = n _{n-1} C _{k-1} \\

3. \sum _{r=k}^{n}{_{r} C _{k}} =  _{n+1} C _{k+1}$$</span><!-- Has MathJax -->

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-03-12T04:00:00.000Z"><a href="/blog/2015/03/12/katex-test-2/">2015-03-12</a></time>
      
      
  
    <h1 class="title"><a href="/blog/2015/03/12/katex-test-2/">Similarity between Backpropagation and Dynamic Programming</a></h1>
  

    </header>
    <div class="entry">
      
        <p>I recently read <a href="http://nlp.stanford.edu/~socherr/sparseAutoencoder_2011new.pdf" target="_blank" rel="external">the lecture notes of Stanford CS294A</a> to learn about autoencoder, but I thought the derivation of partial derivatives for backpropagation algorithm in the lecture notes is a bit unfriendly, so I will try to fill a gap.</p>
<p>On page 7, he describes one iteration of gradient descent updates as follows:<br><span>$$W_{ij}^{(l)} := 
W_{ij}^{(l)} - \alpha \frac{\partial}{\partial W_{ij}^{(l)}}J\left(W,b\right)$$</span><!-- Has MathJax --></p>
<p>We want to find the partial derivative. On the next page, he explains how the backpropagation algorithm can find partial derivatives, but he just set the error term $\delta$ out of nowhere, and it is hard to see where it actually comes from.</p>
<p>We’ll try to see how the error term delta appears from the gradient update equation.</p>
<p>So, the partial derivative in the update equation can be written as follows by chain rule:</p>
<span>$$\frac {\partial J\left(W,b\right)} {\partial W_{ij}^{(l)}}
= \frac {\partial J\left(W,b\right)} {\partial z_{j}^{(l+1)}} \frac {\partial z_{j}^{(l+1)}} {\partial W_{ij}^{(l)}}$$</span><!-- Has MathJax -->
<p>From equation (6) on page 5 $z^{(l+1)} = W^{(l)}a^{(l)} +b^{(l)}$, we can see that the partial derivative Z/W is equal to a. So,</p>
<span>$$\frac {\partial J\left(W,b\right)} {\partial W_{ij}^{(l)}} 
= \frac {\partial J\left(W,b\right)} {\partial z_{j}^{(l+1)}} a_j^{(l)}$$</span><!-- Has MathJax -->
<p>Now, we introduce the delta term by letting<br><span>$$\delta_i^{(l)}
= \frac {\partial J\left(W,b\right)} {\partial z_{i}^{(l)}}
=\frac {\partial J\left(W,b\right)} {\partial a_i^{(l)}} \frac {\partial a_i^{(l)}} {\partial z_{i}^{(l)}}
= \frac {\partial J\left(W,b\right)} {\partial a_i^{(l)}} f&apos;(z_{i}^{(l)})$$</span><!-- Has MathJax --></p>
<p>We look at the partial derivative we get above: $\frac {\partial J\left(W,b\right)} {\partial a_i^{(l)}}$. We can expand this partial derivative by chain rule as follows:</p>
<p>When l = $n_l$:</p>
<span>$$\frac {\partial J\left(W,b\right)} {\partial a_i^{(l)}} = \frac {\partial} {\partial a_{i}^{n_l}}{\frac {1} {2} \left\| y-h_{W,b}(x)\right\|^2}
= -(y_i-a_i^{n_l})$$</span><!-- Has MathJax -->
<p>Note that<br><span>$$a^{n_l} = h_{W,b}(x) \\
J(W,b) ={\frac {1} {2} \left\| y-h_{W,b}(x)\right\|^2}$$</span><!-- Has MathJax --><br>as defined on page 6 in the lecture notes.</p>
<p>When l $\neq n_l$:<br><span>$$\frac {\partial J\left(W,b\right)} {\partial a_{i}^{(l)}} = \frac {\partial J\left(W,b\right)} {\partial z_{1}^{(l+1)}} \frac {\partial z_{1}^{(l+1)}} {\partial a_i^{(l)}} + \frac {\partial J\left(W,b\right)} {\partial z_{2}^{(l+1)}} \frac {\partial z_{2}^{(l+1)}} {\partial a_i^{(l)}} + ... + \frac {\partial J\left(W,b\right)} {\partial z_{s_{l+1}}^{(l+1)}} \frac {\partial z_{s_{l+1}}^{(l+1)}} {\partial a_i^{(l)}}
= \sum _{j=1}^{s_{l+1}}W_{ji}^{(l)}\delta_i^{(l+1)}$$</span><!-- Has MathJax --></p>
<p>Note that from the equation (6) in the lecture notes and as we define earlier,<br><span>$$\frac {\partial J\left(W,b\right)} {\partial z_{j}^{(l+1)}} = W_{ji}^{(l)} \\
\delta_i^{(l+1)} = \frac {\partial J\left(W,b\right)} {\partial z_{i}^{(l+1)}}$$</span><!-- Has MathJax --></p>
<p>The idea behind the derivation for <span>$l &lt; n_l$</span><!-- Has MathJax --> (or actually, the whole algorithm of backpropagation) is sort of similar to dynamic programming. We want to know the partial derivative $\frac {\partial J\left(W,b\right)} {\partial a_{i}^{(l)}}$. That is, we want to know how a small change in $a_i^{(l)}$ might affect the overall cost function $J(W,b)$. In order to find that, we look for the consequences in the layer right after the layer l, which is (l+1). These values in the layer l+1 have to be computed prior to the current node in the layer l, which is $a_i^{(l)}$. Indeed, these values are already computed when we compute the value for $a_i^{(l)}$ because we start with the last layer, and move backward. This is exactly the idea behind dynamic programming.</p>
<p>Finally, from above, we can get to the expressions as the lecture notes have on page 8 in the backpropagation algorithm:<br><span>$$\delta_i^{(n_l)} = -(y_i-a_i^{n_l})f&apos;(z_i^{(n_l)})
\delta_i^{(l)} = \left( \sum _{j=1}^{s_{l+1}}W_{ji}^{(l)}\delta_j^{(l+1)}\right)f&apos;(z_i^{(l)})
\frac {\partial } {\partial W_{ij}^{(l)}} J(W,b) = a_j^{(l)}\delta_i^{(l+1)}$$</span><!-- Has MathJax --></p>
<p>Overall, the reason why the partial derivatives can be represented using the delta term is a bit hard to follow is that the logical flow of the derivation is flipped. The way the lecture notes set the delta term seems magical because it doesn’t explain why we would set the term as it is. The answer is that we just set it because that way the equations (in the derivation) look much simpler (and indeed, by setting the delta term, we can visualize filling up the 2D table, where each entry is a corresponding delta term as we do in dynamic programming.) If the delta term were introduced just for the convenience to make the equations look simpler in the sequence of computation from the update equation, it would have been much easier to follow.</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
    <a href="/blog/page/2/" class="alignleft prev">Anterior</a>
  
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Buscar">
    <input type="hidden" name="q" value="site:runopti.github.io/blog">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">Etiquetas</h3>
  <ul class="entry">
  
    <li><a href="/blog/tags/PRML/">PRML</a><small>1</small></li>
  
    <li><a href="/blog/tags/cv/">cv</a><small>1</small></li>
  
    <li><a href="/blog/tags/decision-theory/">decision-theory</a><small>3</small></li>
  
    <li><a href="/blog/tags/learning-theory/">learning-theory</a><small>2</small></li>
  
    <li><a href="/blog/tags/math/">math</a><small>7</small></li>
  
    <li><a href="/blog/tags/neuralnet/">neuralnet</a><small>9</small></li>
  
    <li><a href="/blog/tags/optimization/">optimization</a><small>5</small></li>
  
    <li><a href="/blog/tags/paper-memo/">paper-memo</a><small>2</small></li>
  
    <li><a href="/blog/tags/research/">research</a><small>0</small></li>
  
    <li><a href="/blog/tags/variational-inference/">variational-inference</a><small>2</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2017 Yutaro Yamada
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/blog/js/jquery.imagesloaded.min.js"></script>
<script src="/blog/js/gallery.js"></script>




<link rel="stylesheet" href="/blog/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/blog/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]],"processEscapes":true}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
